{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CINlXmOiz7ZM",
    "outputId": "2147e1f2-beda-42f7-eb55-6d6119b515d5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.8.1)\n",
      "Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from -r requirements.txt (line 2)) (3.4.8)\n",
      "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.1+cu118)\n",
      "Collecting func_timeout (from -r requirements.txt (line 4))\n",
      "  Downloading func_timeout-4.3.5.tar.gz (44 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.3/44.3 kB\u001B[0m \u001B[31m1.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting otter-grader==1.0.0 (from -r requirements.txt (line 5))\n",
      "  Downloading otter_grader-1.0.0-py3-none-any.whl (163 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m164.0/164.0 kB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.65.0)\n",
      "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.5.3)\n",
      "Collecting transformers==4.26.0 (from -r requirements.txt (line 8))\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m26.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tokenizers==0.13.2 (from -r requirements.txt (line 9))\n",
      "  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.6/7.6 MB\u001B[0m \u001B[31m81.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting datasets==2.9.0 (from -r requirements.txt (line 10))\n",
      "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m462.8/462.8 kB\u001B[0m \u001B[31m44.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting wget (from -r requirements.txt (line 11))\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 5)) (5.9.2)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 5)) (7.34.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 5)) (6.5.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 5)) (67.7.2)\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 5)) (6.3.1)\n",
      "Collecting docker (from otter-grader==1.0.0->-r requirements.txt (line 5))\n",
      "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m148.1/148.1 kB\u001B[0m \u001B[31m20.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 5)) (3.1.2)\n",
      "Collecting dill (from otter-grader==1.0.0->-r requirements.txt (line 5))\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.3/115.3 kB\u001B[0m \u001B[31m16.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pdfkit (from otter-grader==1.0.0->-r requirements.txt (line 5))\n",
      "  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting PyPDF2 (from otter-grader==1.0.0->-r requirements.txt (line 5))\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m232.6/232.6 kB\u001B[0m \u001B[31m27.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 7)) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 7)) (1.23.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0->-r requirements.txt (line 8)) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.26.0->-r requirements.txt (line 8))\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m268.8/268.8 kB\u001B[0m \u001B[31m31.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0->-r requirements.txt (line 8)) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0->-r requirements.txt (line 8)) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0->-r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0->-r requirements.txt (line 10)) (9.0.0)\n",
      "Collecting dill (from otter-grader==1.0.0->-r requirements.txt (line 5))\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m110.5/110.5 kB\u001B[0m \u001B[31m14.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting xxhash (from datasets==2.9.0->-r requirements.txt (line 10))\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m16.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting multiprocess (from datasets==2.9.0->-r requirements.txt (line 10))\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m15.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0->-r requirements.txt (line 10)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0->-r requirements.txt (line 10)) (3.8.5)\n",
      "Collecting responses<0.19 (from datasets==2.9.0->-r requirements.txt (line 10))\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 1)) (8.1.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 3)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->-r requirements.txt (line 3)) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->-r requirements.txt (line 3)) (16.0.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 10)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 10)) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 10)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 10)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 10)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 10)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0->-r requirements.txt (line 8)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0->-r requirements.txt (line 8)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0->-r requirements.txt (line 8)) (2023.7.22)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker->otter-grader==1.0.0->-r requirements.txt (line 5)) (1.6.1)\n",
      "Collecting jedi>=0.16 (from ipython->otter-grader==1.0.0->-r requirements.txt (line 5))\n",
      "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m31.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (3.0.39)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (2.14.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->otter-grader==1.0.0->-r requirements.txt (line 5)) (2.1.3)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.9.0->-r requirements.txt (line 10))\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.3/134.3 kB\u001B[0m \u001B[31m11.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (4.9.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (4.11.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (6.0.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (5.3.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader==1.0.0->-r requirements.txt (line 5)) (2.18.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader==1.0.0->-r requirements.txt (line 5)) (4.3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.19.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (3.10.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (6.1.12)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.2.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 5)) (23.2.1)\n",
      "Building wheels for collected packages: func_timeout, wget\n",
      "  Building wheel for func_timeout (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for func_timeout: filename=func_timeout-4.3.5-py3-none-any.whl size=15078 sha256=0d0eb4bb78f7c089129a82fb4ad6b394633af51713d57a4115066714dafa5994\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/83/19/b5552bb9630e353f7c5b15be44bf10900afe1abbbfcf536afd\n",
      "  Building wheel for wget (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=1de5e65f521ce7b6bb8206e4d341f665818f36ba0f1b818fb132271eb517b8f9\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built func_timeout wget\n",
      "Installing collected packages: wget, tokenizers, pdfkit, func_timeout, xxhash, PyPDF2, jedi, dill, responses, multiprocess, huggingface-hub, docker, transformers, datasets, otter-grader\n",
      "Successfully installed PyPDF2-3.0.1 datasets-2.9.0 dill-0.3.6 docker-6.1.3 func_timeout-4.3.5 huggingface-hub-0.16.4 jedi-0.19.0 multiprocess-0.70.14 otter-grader-1.0.0 pdfkit-1.0.0 responses-0.18.0 tokenizers-0.13.2 transformers-4.26.0 wget-3.2 xxhash-3.3.0\n"
     ]
    }
   ],
   "source": [],
   "id": "CINlXmOiz7ZM"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "27b2bbbb",
    "outputId": "ea7a4ecc-8d6e-49eb-e3ce-b0be1eea77a9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Please do not change this cell because some hidden tests might depend on it.\n",
    "import os\n",
    "\n",
    "# Otter grader does not handle ! commands well, so we define and use our\n",
    "# own function to execute shell commands.\n",
    "def shell(commands, warn=True):\n",
    "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
    "\n",
    "       Prints the result to stdout and returns the exit status.\n",
    "       Provides a printed warning on non-zero exit status unless `warn`\n",
    "       flag is unset.\n",
    "    \"\"\"\n",
    "    file = os.popen(commands)\n",
    "    print (file.read().rstrip('\\n'))\n",
    "    exit_status = file.close()\n",
    "    if warn and exit_status != None:\n",
    "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
    "    return exit_status\n",
    "\n",
    "shell(\"\"\"\n",
    "ls requirements.txt >/dev/null 2>&1\n",
    "if [ ! $? = 0 ]; then\n",
    " rm -rf .tmp\n",
    " git clone https://github.com/cs236299-2023-spring/project4.git .tmp\n",
    " mv .tmp/requirements.txt ./\n",
    " rm -rf .tmp\n",
    "fi\n",
    "pip install -q -r requirements.txt\n",
    "\"\"\")"
   ],
   "id": "27b2bbbb"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fd02e982"
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ],
   "id": "fd02e982"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "d63aa7ef"
   },
   "source": [
    "%%latex\n",
    "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\newcommand{\\Prob}{\\Pr}\n",
    "\\newcommand{\\given}{\\,|\\,}"
   ],
   "id": "d63aa7ef"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2557ead"
   },
   "source": [
    "$$\n",
    "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\renewcommand{\\Prob}{\\Pr}\n",
    "\\renewcommand{\\given}{\\,|\\,}\n",
    "$$"
   ],
   "id": "c2557ead"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LyBi8Jb0t26",
    "outputId": "8169188f-2b4b-445b-a86f-7f52be165adc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "shell(\"\"\"pip install wget\"\"\")"
   ],
   "id": "0LyBi8Jb0t26"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57f766dc"
   },
   "source": [
    "# \t236299 - Introduction to Natural Language Processing\n",
    "## Project 4: Semantic Interpretation – Question Answering"
   ],
   "id": "57f766dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cf2a34b"
   },
   "source": [
    "The goal of semantic parsing is to convert natural language utterances to a meaning representation such as a _logical form_ expression or a _SQL query_. In the previous project segment, you built a parsing system to reconstruct parse trees from the natural-language queries in the ATIS dataset. However, that only solves an intermediary task, not the end-user task of obtaining answers to the queries.\n",
    "\n",
    "In this final project segment, you will go further, building a semantic parsing system to convert English queries to SQL queries, so that by consulting a database you will be able to answer those questions. You will implement both a rule-based approach and an end-to-end sequence-to-sequence (seq2seq) approach. Both algorithms come with their pros and cons, and by the end of this segment you should have a basic understanding of the characteristics of the two approaches.\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. Build a semantic parsing algorithm to convert text to SQL queries based on the syntactic parse trees from the last project.\n",
    "2. Build an attention-based end-to-end seq2seq system to convert text to SQL.\n",
    "3. Improve the attention-based end-to-end seq2seq system with self-attention to convert text to SQL.\n",
    "4. Discuss the pros and cons of the rule-based system and the end-to-end system.\n",
    "5. (Optional) Use the state-of-the-art pretrained transformers for text-to-SQL conversion.\n",
    "\n",
    "This will be an extremely challenging project, so we recommend that you start early."
   ],
   "id": "7cf2a34b"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d345044b"
   },
   "source": [
    "# Setup"
   ],
   "id": "d345044b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5e6883c2"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import wget\n",
    "import nltk\n",
    "import sqlite3\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import Regex\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit, Split\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from cryptography.fernet import Fernet\n",
    "from func_timeout import func_set_timeout\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from tqdm import tqdm\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ],
   "id": "5e6883c2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9311107",
    "outputId": "a2b797ba-71a4-4e9c-c538-33f8c654820c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "# Set timeout for executing SQL\n",
    "TIMEOUT = 3 # seconds\n",
    "\n",
    "# GPU check: Set runtime type to use GPU where available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ],
   "id": "b9311107"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8fe1c8d5"
   },
   "outputs": [],
   "source": [
    "## Download needed scripts and data\n",
    "def download_if_needed(source, dest, filename):\n",
    "    os.path.exists(f\"./{dest}{filename}\") or wget.download(source + filename, out=dest)\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('scripts', exist_ok=True)\n",
    "source_url = \"https://raw.githubusercontent.com/nlp-236299/data/master\"\n",
    "\n",
    "# Grammar to augment for this segment\n",
    "if not os.path.isfile('data/grammar'):\n",
    "  download_if_needed(source_url, \"data/\", \"/ATIS/grammar_distrib4.crypt\")\n",
    "\n",
    "  # Decrypt the grammar file\n",
    "  key = b'bfksTY2BJ5VKKK9xZb1PDDLaGkdu7KCDFYfVePSEfGY='\n",
    "  fernet = Fernet(key)\n",
    "  with open('./data/grammar_distrib4.crypt', 'rb') as f:\n",
    "    restored = Fernet(key).decrypt(f.read())\n",
    "  with open('./data/grammar', 'wb') as f:\n",
    "    f.write(restored)\n",
    "\n",
    "# Download scripts and ATIS database\n",
    "download_if_needed(source_url, \"scripts/\", \"/scripts/trees/transform.py\")\n",
    "download_if_needed(source_url, \"data/\", \"/ATIS/atis_sqlite.db\")"
   ],
   "id": "8fe1c8d5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "26b54859"
   },
   "outputs": [],
   "source": [
    "# Import downloaded scripts for parsing augmented grammars\n",
    "sys.path.insert(1, './scripts')\n",
    "import transform as xform"
   ],
   "id": "26b54859"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ae67727"
   },
   "source": [
    "# Semantically augmented grammars\n",
    "\n",
    "In the first part of this project segment, you'll be implementing a rule-based system for semantic interpretation of sentences. Before jumping into using such a system on the ATIS dataset – we'll get to that soon enough – let's first work with some trivial examples to get things going.\n",
    "\n",
    "The fundamental idea of rule-based semantic interpretation is the rule of compositionality, that *the meaning of a constituent is a function of the meanings of its immediate subconstituents and the syntactic rule that combined them*. This leads to an infrastructure for specifying semantic interpretation in which each syntactic rule in a grammar (in our case, a context-free grammar) is associated with a semantic rule that applies to the meanings associated with the elements on the right-hand side of the rule.\n",
    "\n",
    "## Example: arithmetic expressions\n",
    "\n",
    "As a first example, let's consider an augmented grammar for arithmetic expressions, familiar from lab 3-1. We again use the function `xform.parse_augmented_grammar` to parse the augmented grammar. You can read more about it in the file `scripts/transform.py`."
   ],
   "id": "7ae67727"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ed34d375"
   },
   "outputs": [],
   "source": [
    "arithmetic_grammar, arithmetic_augmentations = xform.parse_augmented_grammar(\n",
    "    \"\"\"\n",
    "    ## Sample grammar for arithmetic expressions\n",
    "\n",
    "    S -> NUM                              : lambda Num: Num\n",
    "       | S OP S                           : lambda S1, Op, S2: Op(S1, S2)\n",
    "\n",
    "    OP -> ADD                             : lambda Op: Op\n",
    "        | SUB\n",
    "        | MULT\n",
    "        | DIV\n",
    "\n",
    "    NUM -> 'zero'                         : lambda: 0\n",
    "         | 'one'                          : lambda: 1\n",
    "         | 'two'                          : lambda: 2\n",
    "         | 'three'                        : lambda: 3\n",
    "         | 'four'                         : lambda: 4\n",
    "         | 'five'                         : lambda: 5\n",
    "         | 'six'                          : lambda: 6\n",
    "         | 'seven'                        : lambda: 7\n",
    "         | 'eight'                        : lambda: 8\n",
    "         | 'nine'                         : lambda: 9\n",
    "         | 'ten'                          : lambda: 10\n",
    "\n",
    "    ADD -> 'plus' | 'added' 'to'          : lambda: lambda x, y: x + y\n",
    "    SUB -> 'minus'                        : lambda: lambda x, y: x - y\n",
    "    MULT -> 'times' | 'multiplied' 'by'   : lambda: lambda x, y: x * y\n",
    "    DIV -> 'divided' 'by'                 : lambda: lambda x, y: x / y\n",
    "    \"\"\"\n",
    ")"
   ],
   "id": "ed34d375"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d6098df"
   },
   "source": [
    "Recall that in this grammar specification format, rules that are not explicitly provided with an augmentation (like all the `OP` rules after the first `OP -> ADD`) are associated with the textually most recent one (`lambda Op: Op`).\n",
    "\n",
    "The `parse_augmented_grammar` function returns both an NLTK grammar and a dictionary that maps from productions in the grammar to their associated augmentations. Let's examine the returned grammar."
   ],
   "id": "1d6098df"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fb4a8dc",
    "outputId": "23463464-fe69-44dd-ad83-f0e0b53eda70"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S -> NUM                      <function <lambda> at 0x7a5f8c05f2e0>\n",
      "S -> S OP S                   <function <lambda> at 0x7a5f8c05f250>\n",
      "OP -> ADD                     <function <lambda> at 0x7a5f8c05f370>\n",
      "OP -> SUB                     <function <lambda> at 0x7a5f8c05f400>\n",
      "OP -> MULT                    <function <lambda> at 0x7a5f8c05f490>\n",
      "OP -> DIV                     <function <lambda> at 0x7a5f8c05f520>\n",
      "NUM -> 'zero'                 <function <lambda> at 0x7a5f8c05f5b0>\n",
      "NUM -> 'one'                  <function <lambda> at 0x7a5f8c05f640>\n",
      "NUM -> 'two'                  <function <lambda> at 0x7a5f8c05f6d0>\n",
      "NUM -> 'three'                <function <lambda> at 0x7a5f8c05f760>\n",
      "NUM -> 'four'                 <function <lambda> at 0x7a5f8c05f7f0>\n",
      "NUM -> 'five'                 <function <lambda> at 0x7a5f8c05f880>\n",
      "NUM -> 'six'                  <function <lambda> at 0x7a5f8c05f910>\n",
      "NUM -> 'seven'                <function <lambda> at 0x7a5f8c05f9a0>\n",
      "NUM -> 'eight'                <function <lambda> at 0x7a5f8c05fa30>\n",
      "NUM -> 'nine'                 <function <lambda> at 0x7a5f8c05fac0>\n",
      "NUM -> 'ten'                  <function <lambda> at 0x7a5f8c05fb50>\n",
      "ADD -> 'plus'                 <function <lambda> at 0x7a5f8c05fbe0>\n",
      "ADD -> 'added' 'to'           <function <lambda> at 0x7a5f8c05fc70>\n",
      "SUB -> 'minus'                <function <lambda> at 0x7a5f8c05fd00>\n",
      "MULT -> 'times'               <function <lambda> at 0x7a5f8c05fd90>\n",
      "MULT -> 'multiplied' 'by'     <function <lambda> at 0x7a5f8c05fe20>\n",
      "DIV -> 'divided' 'by'         <function <lambda> at 0x7a5f8c05feb0>\n"
     ]
    }
   ],
   "source": [
    "for production in arithmetic_grammar.productions():\n",
    "  print(f\"{repr(production):25}     {arithmetic_augmentations[production]}\")"
   ],
   "id": "8fb4a8dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355ed6c4"
   },
   "source": [
    "We can parse with the grammar using one of the built-in NLTK parsers."
   ],
   "id": "355ed6c4"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b04e6b2",
    "outputId": "e7e1c361-517b-4b22-cfb6-a372fa5a4afa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            S            \n",
      "        ____|_________    \n",
      "       S         |    |  \n",
      "   ____|____     |    |   \n",
      "  S    OP   S    OP   S  \n",
      "  |    |    |    |    |   \n",
      " NUM  ADD  NUM  MULT NUM \n",
      "  |    |    |    |    |   \n",
      "three plus one times four\n",
      "\n",
      "            S            \n",
      "   _________|____         \n",
      "  |    |         S       \n",
      "  |    |     ____|____    \n",
      "  S    OP   S    OP   S  \n",
      "  |    |    |    |    |   \n",
      " NUM  ADD  NUM  MULT NUM \n",
      "  |    |    |    |    |   \n",
      "three plus one times four\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arithmetic_parser = nltk.parse.BottomUpChartParser(arithmetic_grammar)\n",
    "parses = [p for p in arithmetic_parser.parse('three plus one times four'.split())]\n",
    "for parse in parses:\n",
    "  parse.pretty_print()"
   ],
   "id": "0b04e6b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68c38950"
   },
   "source": [
    "Now let's turn to the augmentations. They can be arbitrary Python functions applied to the semantic representations associated with the right-hand-side nonterminals, returning the semantic representation of the left-hand side. To interpret the semantic representation of the entire sentence (at the root of the parse tree), we can use the following pseudo-code:\n",
    "```\n",
    "to interpret a tree:\n",
    "  interpret each of the nonterminal-rooted subtrees\n",
    "  find the augmentation associated with the root production of the tree\n",
    "    (it should be a function of as many arguments as there are nonterminals on the right-hand side)\n",
    "  return the result of applying the augmentation to the subtree values\n",
    "```\n",
    "(The base case of this recursion occurs when the number of nonterminal-rooted subtrees is zero, that is, a rule all of whose right-hand side elements are terminals.)\n",
    "\n",
    "Suppose we had such a function, call it `interpret`. How would it operate on, for instance, the tree `(S (S (NUM three)) (OP (ADD plus)) (S (NUM one)))`?\n",
    "\n",
    "```\n",
    "interpret (S (S (NUM three)) (OP (ADD plus)) (S (NUM one)))\n",
    "    |->interpret (S (NUM three))\n",
    "    |      |->interpret (NUM three)\n",
    "    |      |      |->(no subconstituents to evaluate)\n",
    "    |      |      |->apply the augmentation for the rule NUM -> three to the empty set of values\n",
    "    |      |      |      (lambda: 3) () ==> 3\n",
    "    |      |      \\==> 3\n",
    "    |      |->apply the augmentation for the rule S -> NUM to the value 3\n",
    "    |      |      (lambda NUM: NUM)(3) ==> 3\n",
    "    |      \\==> 3\n",
    "    |->interpret (OP (ADD plus))\n",
    "    |      |...\n",
    "    |      \\==> lambda x, y: x + y\n",
    "    |->interpret (S (NUM one))\n",
    "    |      |...\n",
    "    |      \\==> 1\n",
    "    |->apply the augmentation for the rule S -> S OP S to the values 3, (lambda x, y: x + y), and 1\n",
    "    |      (lambda S1, Op, S2: Op(S1, S2))(3, (lambda x, y: x + y), 1) ==> 4\n",
    "    \\==> 4\n",
    "```\n",
    "\n",
    "Thus, the string \"three plus one\" is semantically interpreted as the value 4.\n",
    "\n",
    "We provide the `interpret` function to carry out this recursive process, copied over from lab 4-2:"
   ],
   "id": "68c38950"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "99104630"
   },
   "outputs": [],
   "source": [
    "def interpret(tree, augmentations):\n",
    "  syntactic_rule = tree.productions()[0]\n",
    "  semantic_rule = augmentations[syntactic_rule]\n",
    "  child_meanings = [interpret(child, augmentations)\n",
    "                    for child in tree\n",
    "                    if isinstance(child, nltk.Tree)]\n",
    "  return semantic_rule(*child_meanings)"
   ],
   "id": "99104630"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "622fb2dc"
   },
   "source": [
    "Now we should be able to evaluate the arithmetic example from above."
   ],
   "id": "622fb2dc"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "855a5c99",
    "outputId": "c6f36911-dcaf-4722-caf8-e4ca84ecd237"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "interpret(parses[0], arithmetic_augmentations)"
   ],
   "id": "855a5c99"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f348bc8"
   },
   "source": [
    "And we can even write a function that parses and interprets a string. We'll have it evaluate each of the possible parses and print the results."
   ],
   "id": "6f348bc8"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dd784592"
   },
   "outputs": [],
   "source": [
    "def parse_and_interpret(string, grammar, augmentations):\n",
    "  parser = nltk.parse.BottomUpChartParser(grammar)\n",
    "  parses = parser.parse(string.split())\n",
    "  for parse in parses:\n",
    "    parse.pretty_print()\n",
    "    print(parse, \"==>\", interpret(parse, augmentations))"
   ],
   "id": "dd784592"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66a97c1c",
    "outputId": "2d2ad68a-f7cd-4f61-b28a-47f02c2f96a0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            S            \n",
      "        ____|_________    \n",
      "       S         |    |  \n",
      "   ____|____     |    |   \n",
      "  S    OP   S    OP   S  \n",
      "  |    |    |    |    |   \n",
      " NUM  ADD  NUM  MULT NUM \n",
      "  |    |    |    |    |   \n",
      "three plus one times four\n",
      "\n",
      "(S\n",
      "  (S (S (NUM three)) (OP (ADD plus)) (S (NUM one)))\n",
      "  (OP (MULT times))\n",
      "  (S (NUM four))) ==> 16\n",
      "            S            \n",
      "   _________|____         \n",
      "  |    |         S       \n",
      "  |    |     ____|____    \n",
      "  S    OP   S    OP   S  \n",
      "  |    |    |    |    |   \n",
      " NUM  ADD  NUM  MULT NUM \n",
      "  |    |    |    |    |   \n",
      "three plus one times four\n",
      "\n",
      "(S\n",
      "  (S (NUM three))\n",
      "  (OP (ADD plus))\n",
      "  (S (S (NUM one)) (OP (MULT times)) (S (NUM four)))) ==> 7\n"
     ]
    }
   ],
   "source": [
    "parse_and_interpret(\"three plus one times four\", arithmetic_grammar, arithmetic_augmentations)"
   ],
   "id": "66a97c1c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6231db03"
   },
   "source": [
    "Since the string is syntactically ambiguous according to the grammar, it is semantically ambiguous as well."
   ],
   "id": "6231db03"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f065c35f"
   },
   "source": [
    "## Some grammar specification conveniences\n",
    "\n",
    "Before going on, it will be useful to have a few more conveniences in writing augmentations for rules. First, since the augmentations are arbitrary Python expressions, they can be built from and make use of other functions. For instance, you'll notice that many of the augmentations at the leaves of the tree took no arguments and returned a constant. We can define a function `constant` that returns a function that ignores its arguments and returns a particular value."
   ],
   "id": "f065c35f"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8b8b3fea"
   },
   "outputs": [],
   "source": [
    "def constant(value):\n",
    "  \"\"\"Return `value`, ignoring any arguments\"\"\"\n",
    "  return lambda *args: value"
   ],
   "id": "8b8b3fea"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72a031be"
   },
   "source": [
    "Similarly, several of the augmentations are functions that just return their first argument. Again, we can define a generic form `first` of such a function:"
   ],
   "id": "72a031be"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "103a6d09"
   },
   "outputs": [],
   "source": [
    "def first(*args):\n",
    "  \"\"\"Return the value of the first (and perhaps only) subconstituent,\n",
    "     ignoring any others\"\"\"\n",
    "  return args[0]"
   ],
   "id": "103a6d09"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a433f17"
   },
   "source": [
    "We can now rewrite the grammar above to take advantage of these shortcuts.\n",
    "\n",
    "> In the call to `parse_augmented_grammar` below, we pass in the global environment, extracted via a `globals()` function call, via the named argument `globals`. This allows the `parse_augmented_grammar` function to make use of the global bindings for `constant`, `first`, and the like when evaluating the augmentation expressions to their values. You can check out the code in `transform.py` to see how the passed in `globals` bindings are used. To help understand what's going on, see what happens if you don't include the `globals=globals()`."
   ],
   "id": "0a433f17"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6450c6e3"
   },
   "outputs": [],
   "source": [
    "arithmetic_grammar_2, arithmetic_augmentations_2 = xform.parse_augmented_grammar(\n",
    "    \"\"\"\n",
    "    ## Sample grammar for arithmetic expressions\n",
    "\n",
    "    S -> NUM                              : first\n",
    "       | S OP S                           : lambda S1, Op, S2: Op(S1, S2)\n",
    "\n",
    "    OP -> ADD                             : first\n",
    "       | SUB\n",
    "       | MULT\n",
    "       | DIV\n",
    "\n",
    "    NUM -> 'zero'                         : constant(0)\n",
    "         | 'one'                          : constant(1)\n",
    "         | 'two'                          : constant(2)\n",
    "         | 'three'                        : constant(3)\n",
    "         | 'four'                         : constant(4)\n",
    "         | 'five'                         : constant(5)\n",
    "         | 'six'                          : constant(6)\n",
    "         | 'seven'                        : constant(7)\n",
    "         | 'eight'                        : constant(8)\n",
    "         | 'nine'                         : constant(9)\n",
    "         | 'ten'                          : constant(10)\n",
    "\n",
    "    ADD -> 'plus' | 'added' 'to'          : constant(lambda x, y: x + y)\n",
    "    SUB -> 'minus'                        : constant(lambda x, y: x - y)\n",
    "    MULT -> 'times' | 'multiplied' 'by'   : constant(lambda x, y: x * y)\n",
    "    DIV -> 'divided' 'by'                 : constant(lambda x, y: x / y)\n",
    "    \"\"\",\n",
    "    globals=globals())"
   ],
   "id": "6450c6e3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e01f958"
   },
   "source": [
    "Finally, it might make our lives easier to write a template of augmentations whose instantiation depends on the right-hand side of the rule.\n",
    "\n",
    "We use a reserved keyword `_RHS` to denote the right-hand side of the syntactic rule, which will be replaced by a **list** of the right-hand-side strings. For example, an augmentation `numeric_template(_RHS)` would be as if written as `numeric_template(['zero'])` when the rule is `NUM -> 'zero'`, and `numeric_template(['one'])` when the rule is `NUM -> 'one'`. The details of how this works can be found at [`scripts/transform.py`](https://github.com/nlp-236299/data/blob/master/scripts/trees/transform.py).\n",
    "\n",
    "This would allow us to use a single template function, for example,"
   ],
   "id": "1e01f958"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "a910940e"
   },
   "outputs": [],
   "source": [
    "def numeric_template(rhs):\n",
    "  \"\"\"Ignore the subphrase meanings and lookup the first right-hand-side symbol\n",
    "     as a number\"\"\"\n",
    "  return constant({'zero':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5,\n",
    "          'six':6, 'seven':7, 'eight':8, 'nine':9, 'ten':10}[rhs[0]])"
   ],
   "id": "a910940e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c49ac69"
   },
   "source": [
    "and then further simplify the grammar specification:"
   ],
   "id": "7c49ac69"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "be55160f"
   },
   "outputs": [],
   "source": [
    "arithmetic_grammar_3, arithmetic_augmentations_3 = xform.parse_augmented_grammar(\n",
    "    \"\"\"\n",
    "    ## Sample grammar for arithmetic expressions\n",
    "\n",
    "    S -> NUM                              : first\n",
    "       | S OP S                           : lambda S1, Op, S2: Op(S1, S2)\n",
    "\n",
    "    OP -> ADD                             : first\n",
    "       | SUB\n",
    "       | MULT\n",
    "       | DIV\n",
    "\n",
    "    NUM -> 'zero'  | 'one'   | 'two'      : numeric_template(_RHS)\n",
    "         | 'three' | 'four'  | 'five'\n",
    "         | 'six'   | 'seven' | 'eight'\n",
    "         | 'nine'  | 'ten'\n",
    "\n",
    "    ADD -> 'plus' | 'added' 'to'          : constant(lambda x, y: x + y)\n",
    "    SUB -> 'minus'                        : constant(lambda x, y: x - y)\n",
    "    MULT -> 'times' | 'multiplied' 'by'   : constant(lambda x, y: x * y)\n",
    "    DIV -> 'divided' 'by'                 : constant(lambda x, y: x / y)\n",
    "    \"\"\",\n",
    "    globals=globals())"
   ],
   "id": "be55160f"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1b1267d2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7788f645-d106-49cc-a8dd-eebabc99cffc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       S                 \n",
      "  _____|______________    \n",
      " S           OP       S  \n",
      " |           |        |   \n",
      "NUM         DIV      NUM \n",
      " |      _____|___     |   \n",
      "six divided      by three\n",
      "\n",
      "(S (S (NUM six)) (OP (DIV divided by)) (S (NUM three))) ==> 2.0\n"
     ]
    }
   ],
   "source": [
    "parse_and_interpret(\"six divided by three\", arithmetic_grammar_3, arithmetic_augmentations_3)"
   ],
   "id": "1b1267d2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7610d6de"
   },
   "source": [
    "## Example: _Green Eggs and Ham_ revisited\n",
    "\n",
    "This stuff is tricky, so it's useful to see more examples before jumping in the deep end. In this simple GEaH fragment grammar, we use a larger set of auxiliary functions to build the augmentations."
   ],
   "id": "7610d6de"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "14e7a796"
   },
   "outputs": [],
   "source": [
    "def forward(F, A):\n",
    "  \"\"\"Forward application: Return the application of the first\n",
    "     argument to the second\"\"\"\n",
    "  return F(A)\n",
    "\n",
    "def backward(A, F):\n",
    "  \"\"\"Backward application: Return the application of the second\n",
    "     argument to the first\"\"\"\n",
    "  return F(A)\n",
    "\n",
    "def second(*args):\n",
    "  \"\"\"Return the value of the second subconstituent, ignoring any others\"\"\"\n",
    "  return args[1]\n",
    "\n",
    "def ignore(*args):\n",
    "  \"\"\"Return `None`, ignoring everything about the constituent. (Good as a\n",
    "     placeholder until a better augmentation can be devised.)\"\"\"\n",
    "  return None"
   ],
   "id": "14e7a796"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1335b23d"
   },
   "source": [
    "Using these, we can build and test the grammar."
   ],
   "id": "1335b23d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7defaca8"
   },
   "outputs": [],
   "source": [
    "geah_grammar_spec = \"\"\"\n",
    "  ## Productions\n",
    "  S -> NP VP            : backward\n",
    "  VP -> V NP            : forward\n",
    "\n",
    "  ## Lexicon\n",
    "  V -> 'likes'          : constant(lambda Object: lambda Subject: f\"like({Subject}, {Object})\")\n",
    "  NP -> 'Sam' | 'sam'   : constant(_RHS[0])\n",
    "  NP -> 'ham'\n",
    "  NP -> 'eggs'\n",
    "\"\"\""
   ],
   "id": "7defaca8"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "931fea18"
   },
   "outputs": [],
   "source": [
    "geah_grammar, geah_augmentations = xform.parse_augmented_grammar(geah_grammar_spec,\n",
    "                                                                 globals=globals())"
   ],
   "id": "931fea18"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75d6b0b6",
    "outputId": "f9f2fc37-2037-4e41-80d7-2d2381271a68"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      S          \n",
      "  ____|____       \n",
      " |         VP    \n",
      " |     ____|___   \n",
      " NP   V        NP\n",
      " |    |        |  \n",
      "Sam likes     ham\n",
      "\n",
      "(S (NP Sam) (VP (V likes) (NP ham))) ==> like(Sam, ham)\n"
     ]
    }
   ],
   "source": [
    "parse_and_interpret(\"Sam likes ham\", geah_grammar, geah_augmentations)"
   ],
   "id": "75d6b0b6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37c6846d"
   },
   "source": [
    "# Semantics of ATIS queries\n",
    "\n",
    "Now you're in a good position to understand and add augmentations to a more comprehensive grammar, say, one that parses ATIS queries and generates SQL queries.\n",
    "\n",
    "In preparation for that, we need to load the ATIS data, both NL and SQL queries."
   ],
   "id": "37c6846d"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93384b7d"
   },
   "source": [
    "## Loading and preprocessing the corpus\n",
    "\n",
    "To simplify things a bit, we'll only consider ATIS queries whose question type (remember that from project segment 1?) is `flight_id`. We download training, development, and test splits for this subset of the ATIS corpus, including corresponding SQL queries."
   ],
   "id": "93384b7d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4f51262c"
   },
   "outputs": [],
   "source": [
    "# Acquire the datasets - training, development, and test splits of the\n",
    "# ATIS queries and corresponding SQL queries\n",
    "download_if_needed(source_url, \"data/\", \"/ATIS/test_flightid.nl\")\n",
    "download_if_needed(source_url, \"data/\", \"/ATIS/test_flightid.sql\")\n",
    "download_if_needed(source_url, \"data/\", \"/ATIS/dev_flightid.nl\")\n",
    "download_if_needed(source_url, \"data/\", \"/ATIS/dev_flightid.sql\")\n",
    "download_if_needed(source_url, \"data/\", \"/ATIS/train_flightid.nl\")\n",
    "download_if_needed(source_url, \"data/\", \"/ATIS/train_flightid.sql\")"
   ],
   "id": "4f51262c"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "48bcfffc"
   },
   "outputs": [],
   "source": [
    "# Process data\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    src_in_file = f'data/{split}_flightid.nl'\n",
    "    tgt_in_file = f'data/{split}_flightid.sql'\n",
    "    out_file = f'data/{split}.csv'\n",
    "\n",
    "    with open(src_in_file, 'r') as f_src_in, open(tgt_in_file, 'r') as f_tgt_in:\n",
    "        with open(out_file, 'w') as f_out:\n",
    "            src, tgt= [], []\n",
    "            writer = csv.writer(f_out)\n",
    "            writer.writerow(('src','tgt'))\n",
    "            for src_line, tgt_line in zip(f_src_in, f_tgt_in):\n",
    "                writer.writerow((src_line.strip(), tgt_line.strip()))"
   ],
   "id": "48bcfffc"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e87c15b5"
   },
   "source": [
    "Let's take a look at what the data file looks like."
   ],
   "id": "e87c15b5"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46cf3d34",
    "outputId": "79221080-447b-4512-cab8-2dd48512a3ef"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "src,tgt\n",
      "what flights are available tomorrow from denver to philadelphia,\"SELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport_service airport_service_1 , city city_1 , airport_service airport_service_2 , city city_2 , days days_1 , date_day date_day_1 WHERE flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'DENVER' AND ( flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'PHILADELPHIA' AND flight_1.flight_days = days_1.days_code AND days_1.day_name = date_day_1.day_name AND date_day_1.year = 1991 AND date_day_1.month_number = 1 AND date_day_1.day_number = 20 )\"\n"
     ]
    }
   ],
   "source": [
    "shell(\"head -2 data/dev.csv\")"
   ],
   "id": "46cf3d34"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ec9ba80"
   },
   "source": [
    "## Corpus preprocessing"
   ],
   "id": "2ec9ba80"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8489fbcf"
   },
   "source": [
    "We'll use `tokenizers` and `datasets` to process the data. We'll use the NLTK tokenizer from project segment 3."
   ],
   "id": "8489fbcf"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbdd5528",
    "outputId": "ac410a32-ecc0-4d19-f481-826773168bb2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['are', 'there', 'any', 'first-class', 'flights', 'from', 'st.', 'louis', 'at', '11', 'pm', 'for', 'less', 'than', '$3.50', '?']\n"
     ]
    }
   ],
   "source": [
    "## NLTK Tokenizer\n",
    "tokenizer_pattern = '\\d+|st\\.|[\\w-]+|\\$[\\d\\.]+|\\S+'\n",
    "nltk_tokenizer = nltk.tokenize.RegexpTokenizer(tokenizer_pattern)\n",
    "def tokenize_nltk(string):\n",
    "  return nltk_tokenizer.tokenize(string.lower())\n",
    "\n",
    "## Demonstrating the tokenizer\n",
    "## Note especially the handling of `\"11pm\"` and hyphenated words.\n",
    "print(tokenize_nltk(\"Are there any first-class flights from St. Louis at 11pm for less than $3.50?\"))"
   ],
   "id": "fbdd5528"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532,
     "referenced_widgets": [
      "4dee71169f684f2e88014fcaf29708f7",
      "1cfa79d944f446dba90cc71fdda893dd",
      "a0d768076952436aa3124db53109eb61",
      "5f430e1bce414d4a93bbf1055969f498",
      "efddd5126bea46508e15099d62ed7873",
      "f56d9316ca5842d1a552e3b40d5f2956",
      "7762395d366744068a5c9dacd2aca400",
      "30bd96363cdf4f2e8569f96b2b03968b",
      "19e33c9b88e5454db4719512ae002cda",
      "ad9654e7d35e4b368ea64912dfea76f8",
      "00354d752dfd4c6f97c9a0a522c02dcd",
      "ae99dd7c4f884f7ab8b83fed15d4d86c",
      "ac79489695f348aca3d355fa5716fb34",
      "950efd1d6326491c9f87b0471e99601c",
      "81fb3951333b41e7acced4bcd370e0da",
      "96e288bbd6c143c7b174ea1fc77e4489",
      "6968fb739679425abff833d4dee61361",
      "c14a234fd84e4a70903e6b0a9ceeb5ae",
      "84c6ee75b7e642b58477da0bd2520049",
      "8195fef7e151400e8719fef71cfa0f49",
      "93aa9223c9014bc9b53e1f5095a25783",
      "a31dea566d904677ad82ce5737934828",
      "df7813c20694472eb4e865d476615d90",
      "b4c4e5cbcf1140a8bfeeda3fc42e0582",
      "e884488e81c040beb57b0af5921a9c3f",
      "30fee2aaf20c4388b5e0c6cceff5b9d0",
      "4c6bf6786cc3498887732e4e4d7edcb9",
      "5bf63a9831114ed59e0b4f655c2b0b94",
      "cbb8257b2914471f82b896927803964f",
      "c2cbf6c4a4e64113b320928c0ed24e5a",
      "07a84d3050684101a4c0d96cf13304f5",
      "be6bb989d048445a9c3fd1d908017f6d",
      "b6b13cdf776243b1b1f6b69324b0eba3",
      "292cc79415624d31927e0c0158804314",
      "2ec338c61b03411883d8d20cb93d4628",
      "b34535e0219143eca809fad32ef90a78",
      "201759a49d4549eda5f8a5eaa430d5b9",
      "1752c94ecb7d4966a32e2037ec8e8eb6",
      "6e880523367e4cc290e70c1d04e76187",
      "18e41402e7b040e3bc404496102e3000",
      "73388a6ca11c45399980dc8a7c3bd78c",
      "7ef0f1e963c84205a74d08b544c6de61",
      "4ff987d03525421183945dbbc0b2581a",
      "8cf2a421d5ea42ff805d481a63c1dd44",
      "db80abd25c674a6680f3adef61315673",
      "3fc84d0d8eb84763920785ff056296fc",
      "8c37a61a29614dadafa95f6c66d8eea6",
      "ae7d3f2982c54a98ba1584501d98d35e",
      "e7fa8cd383d54aa787373e0adfc27be3",
      "b4350273089f47ed97f839ad0473d65e",
      "3ffb61197118488585f3bc58f4ac5782",
      "9d6cd59136864119b69168c83fd86d8f",
      "5057b43ed0dc4a3692a3473198c28c2c",
      "1035fdd02a964e34960a9ccacd1c19e9",
      "be0086dbebfd40fc889847138ddf3cde",
      "f8e39e0ac7b64dbe9698cd9880b607e2",
      "8d030d51c3534ab4ac79687bff877e87",
      "d023c306c6b54da5a351bf9c6c6c8bdc",
      "c4302cb2529c479ab1a058cf92f8543c",
      "8ee814c68db04b94bbde751a8a332ab6",
      "cc22ebb9b7a14670ac4932473f3c0de2",
      "8dc0fac98db8441f96ce3679847b53c6",
      "884607c5e45d4ad6a501e33d9f12b002",
      "b7c50a5c7b524c34a3583dde2350f17c",
      "aaed3ae2aa714dc8b7ab14b241527da3",
      "d31354e47a5e44e4bc53abec349160d2"
     ]
    },
    "id": "a6179744",
    "outputId": "375de72b-05a9-49cf-f364-63827b21e817"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-bad314218b37a3f1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-bad314218b37a3f1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4dee71169f684f2e88014fcaf29708f7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae99dd7c4f884f7ab8b83fed15d4d86c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df7813c20694472eb4e865d476615d90"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "292cc79415624d31927e0c0158804314"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db80abd25c674a6680f3adef61315673"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-bad314218b37a3f1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8e39e0ac7b64dbe9698cd9880b607e2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['src', 'tgt'],\n",
       "        num_rows: 3651\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['src', 'tgt'],\n",
       "        num_rows: 398\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['src', 'tgt'],\n",
       "        num_rows: 332\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files={'train':f'data/train.csv', \\\n",
    "                                          'val': f'data/dev.csv', \\\n",
    "                                          'test': f'data/test.csv'})\n",
    "dataset"
   ],
   "id": "a6179744"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "cf2002eb"
   },
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "val_data = dataset['val']\n",
    "test_data = dataset['test']"
   ],
   "id": "cf2002eb"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dbdb6bd0"
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 3\n",
    "unk_token = '[UNK]'\n",
    "pad_token = '[PAD]'\n",
    "bos_token = '<bos>'\n",
    "eos_token = '<eos>'\n",
    "\n",
    "src_tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n",
    "src_tokenizer.normalizer = normalizers.Lowercase()\n",
    "src_tokenizer.pre_tokenizer = Split(Regex(tokenizer_pattern), behavior='removed', invert=True)\n",
    "\n",
    "src_trainer = WordLevelTrainer(min_frequency=MIN_FREQ, special_tokens=[pad_token, unk_token])\n",
    "src_tokenizer.train_from_iterator(train_data['src'], trainer=src_trainer)\n",
    "\n",
    "tgt_tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n",
    "tgt_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "\n",
    "tgt_trainer = WordLevelTrainer(min_frequency=MIN_FREQ, special_tokens=[pad_token, unk_token, bos_token, eos_token])\n",
    "\n",
    "tgt_tokenizer.train_from_iterator(train_data['tgt'], trainer=tgt_trainer)\n",
    "\n",
    "tgt_tokenizer.post_processor = TemplateProcessing(single=f\"{bos_token} $A {eos_token}\", special_tokens=[(bos_token, tgt_tokenizer.token_to_id(bos_token)), (eos_token, tgt_tokenizer.token_to_id(eos_token))])"
   ],
   "id": "dbdb6bd0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76b6fbcd"
   },
   "source": [
    "Note that we prepended `<bos>` and appended `<eos>` to target sentences.\n",
    "\n",
    "We use `datasets.Dataset.map` to convert text into word ids. As shown in lab 1-5, first we need to wrap `tokenizer` with the `transformers.PreTrainedTokenizerFast` class to be compatible with the `datasets` library."
   ],
   "id": "76b6fbcd"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "522f3e8901974611ae4b189ce991bfbc",
      "4cebe6d2681846c79a438cdd5b29a661",
      "863bed341ca34d098ae52e6b8db33040",
      "6f209b9ecc3d4baa8f22519cff0954e9",
      "2bc42955d4334e5c82f5a99a95eb08f8",
      "c678a562d4534be297137ce9871eaa90",
      "0d828a29493c4bb7a78e8eaa88bbdb94",
      "5471c56871bf4cc089b61a7dde8c050a",
      "5126ccc1ca3145748fff13916cfbec2c",
      "ecb0aa5b1af643e8aa9c2d0d30358bca",
      "9958ba66414d43df874eb3123932068a",
      "1343a07fc8a348d3aa8fe0f47e52db8b",
      "2e4fbc92ad3b48ff8615332a6eda4a8e",
      "1c1f5e914f374b99807fca96139fb028",
      "cd94788d00714fddb7a1f38d2e31b79d",
      "0a36f7df7d9d4253a626123d3a2ff3a3",
      "e8aa163c29704d5889bbc5fdec44634d",
      "7f6f923d36854bf294245ae4b08b5d36",
      "72ae0700c19447289b8512ebe699ed55",
      "d1c80f56403a4d9d8a3191c4fdd52a1c",
      "254b957a206c48a3978039a9b30f2f94",
      "468d6e3b9ea7425fa0e37a828e9cfadb",
      "4fd96144edd941779e31d3fb807ca7df",
      "82997ab5b88345b2bfda4a3eb49aa34d",
      "c553be0486d04e49b232984b9b9a0eb2",
      "d838801205664038974c26a4f88fa76c",
      "2903812e68b94a3ba9d78673bb785d0f",
      "dcf2a64d93694d2083fe229391b480d2",
      "62e11e1cffa14e658f3449adcc46fe65",
      "a9ff18e6f8da409c8f0d17d07abecde2",
      "63e0005b21304a4d92b36182bdec023b",
      "713e31dbab5b4625a0f108af261ae37f",
      "5d595b58fb5f492897ebf48388c2554c"
     ]
    },
    "id": "0646660f",
    "outputId": "409217d3-fda0-4327-cff8-d9562fdf99cb"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/3651 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "522f3e8901974611ae4b189ce991bfbc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/398 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1343a07fc8a348d3aa8fe0f47e52db8b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/332 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fd96144edd941779e31d3fb807ca7df"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "hf_src_tokenizer = PreTrainedTokenizerFast(tokenizer_object=src_tokenizer, pad_token=pad_token, unk_token=unk_token)\n",
    "hf_tgt_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tgt_tokenizer, pad_token=pad_token, unk_token=unk_token, bos_token=bos_token, eos_token=eos_token)\n",
    "\n",
    "def encode(example):\n",
    "    example['src_ids'] = hf_src_tokenizer(example['src']).input_ids\n",
    "    example['tgt_ids'] = hf_tgt_tokenizer(example['tgt']).input_ids\n",
    "    return example\n",
    "\n",
    "train_data = train_data.map(encode)\n",
    "val_data = val_data.map(encode)\n",
    "test_data = test_data.map(encode)"
   ],
   "id": "0646660f"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2dcadb0",
    "outputId": "0093867b-866a-4161-fb17-25caa5fc6c06"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of English vocab: 421\n",
      "Size of SQL vocab: 392\n",
      "Index for src padding: 0\n",
      "Index for tgt padding: 0\n",
      "Index for start of sequence token: 2\n",
      "Index for end of sequence token: 3\n"
     ]
    }
   ],
   "source": [
    "# Compute size of vocabulary\n",
    "src_vocab = src_tokenizer.get_vocab()\n",
    "tgt_vocab = tgt_tokenizer.get_vocab()\n",
    "\n",
    "print(f\"Size of English vocab: {len(src_vocab)}\")\n",
    "print(f\"Size of SQL vocab: {len(tgt_vocab)}\")\n",
    "print(f\"Index for src padding: {src_vocab[pad_token]}\")\n",
    "print(f\"Index for tgt padding: {tgt_vocab[pad_token]}\")\n",
    "print(f\"Index for start of sequence token: {tgt_vocab[bos_token]}\")\n",
    "print(f\"Index for end of sequence token: {tgt_vocab[eos_token]}\")"
   ],
   "id": "c2dcadb0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae99d64d"
   },
   "source": [
    "Next, we batch our data to facilitate processing on a GPU.\n",
    "Batching is a bit tricky because the source and target will typically be of different lengths and have to padd the sequences to the same length.\n",
    "Since there is  padding, we need to handle them with [`pack`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence) and [`unpack`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html) later on in the seq2seq part (as in lab 4-5)."
   ],
   "id": "ae99d64d"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "b67bbe04"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16     # batch size for training and validation\n",
    "TEST_BATCH_SIZE = 1 # batch size for test; we use 1 to make implementation easier\n",
    "\n",
    "# Defines how to batch a list of examples together\n",
    "def collate_fn(examples):\n",
    "    batch = {}\n",
    "    bsz = len(examples)\n",
    "    src_ids, tgt_ids = [], []\n",
    "    for example in examples:\n",
    "        src_ids.append(example['src_ids'])\n",
    "        tgt_ids.append(example['tgt_ids'])\n",
    "\n",
    "    src_len = torch.LongTensor([len(word_ids) for word_ids in src_ids]).to(device)\n",
    "    src_max_length = max(src_len)\n",
    "    tgt_max_length = max([len(word_ids) for word_ids in tgt_ids])\n",
    "\n",
    "    src_batch = torch.zeros(bsz, src_max_length).long().fill_(src_vocab[pad_token]).to(device)\n",
    "    tgt_batch = torch.zeros(bsz, tgt_max_length).long().fill_(tgt_vocab[pad_token]).to(device)\n",
    "    for b in range(bsz):\n",
    "        src_batch[b][:len(src_ids[b])] = torch.LongTensor(src_ids[b]).to(device)\n",
    "        tgt_batch[b][:len(tgt_ids[b])] = torch.LongTensor(tgt_ids[b]).to(device)\n",
    "\n",
    "    batch['src_lengths'] = src_len\n",
    "    batch['src_ids'] = src_batch\n",
    "    batch['tgt_ids'] = tgt_batch\n",
    "    return batch\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_data,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True,\n",
    "                                         collate_fn=collate_fn)\n",
    "val_iter = torch.utils.data.DataLoader(val_data,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       shuffle=False,\n",
    "                                       collate_fn=collate_fn)\n",
    "test_iter = torch.utils.data.DataLoader(test_data,\n",
    "                                        batch_size=TEST_BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                        collate_fn=collate_fn)"
   ],
   "id": "b67bbe04"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f4b3ac6"
   },
   "source": [
    "Let's look at a single batch from one of these iterators."
   ],
   "id": "9f4b3ac6"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b54608c0",
    "outputId": "1aa46b12-fedd-4931-e444-e6c1488e757a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of text batch: torch.Size([16, 30])\n",
      "Third sentence in batch: tensor([ 9,  7,  4,  3, 13, 16,  2, 11,  6, 69,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n",
      "Length of the third sentence in batch: 30\n",
      "Converted back to string: show me flights from san francisco to boston on thursday [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Size of sql batch: torch.Size([16, 153])\n",
      "Third sql in batch: tensor([  2,  14,  31,  11,  13,  12,  16,   6,   7,  22,   6,   8,  23,   6,\n",
      "          7,  29,   6,   8,  30,   6,  33,  40,   6,  38,  46,  15,  21,   4,\n",
      "         18,   5,  19,   4,  17,   5,  20,   4,  54,  56,   5,   9,  24,   4,\n",
      "         25,   5,  26,   4,  27,   5,  28,   4,  52,   5,  34,   4,  36,   5,\n",
      "         37,   4,  41,   5,  44,   4,  35,   5,  43,   4, 103,   5,  42,   4,\n",
      "        126,  10,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "       device='cuda:0')\n",
      "Converted back to string: <bos> SELECT DISTINCT flight_1.flight_id FROM flight flight_1, airport_service airport_service_1, city city_1, airport_service airport_service_2, city city_2, days days_1, date_day date_day_1 WHERE flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'SAN FRANCISCO' AND ( flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'BOSTON' AND flight_1.flight_days = days_1.days_code AND days_1.day_name = date_day_1.day_name AND date_day_1.year = 1991 AND date_day_1.month_number = 5 AND date_day_1.day_number = 24 ) <eos> [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "src_ids = batch['src_ids']\n",
    "src_example = src_ids[2]\n",
    "print (f\"Size of text batch: {src_ids.size()}\")\n",
    "print (f\"Third sentence in batch: {src_example}\")\n",
    "print (f\"Length of the third sentence in batch: {len(src_example)}\")\n",
    "print (f\"Converted back to string: {hf_src_tokenizer.decode(src_example)}\")\n",
    "\n",
    "tgt_ids = batch['tgt_ids']\n",
    "tgt_example = tgt_ids[2]\n",
    "print (f\"Size of sql batch: {tgt_ids.size()}\")\n",
    "print (f\"Third sql in batch: {tgt_example}\")\n",
    "print (f\"Converted back to string: {hf_tgt_tokenizer.decode(tgt_example)}\")"
   ],
   "id": "b54608c0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87362e88"
   },
   "source": [
    "Alternatively, we can directly iterate over the raw examples:"
   ],
   "id": "87362e88"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51eec2bf",
    "outputId": "61ba078f-3961-4950-e6e7-e447a2cafe14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Question: list all the flights that arrive at general mitchell international from various cities\n",
      "\n",
      "SQL: SELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport airport_1 , airport_service airport_service_1 , city city_1 WHERE flight_1.to_airport = airport_1.airport_code AND airport_1.airport_code = 'MKE' AND flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND 1 = 1\n"
     ]
    }
   ],
   "source": [
    "for _, example in zip(range(1), train_data):\n",
    "  train_text_1 = example['src'] # detokenized question\n",
    "  train_sql_1 = example['tgt']  # detokenized sql\n",
    "  print (f\"Question: {train_text_1}\\n\")\n",
    "  print (f\"SQL: {train_sql_1}\")"
   ],
   "id": "51eec2bf"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd104480"
   },
   "source": [
    "## Establishing a SQL database for evaluating ATIS queries\n",
    "\n",
    "The output of our systems will be SQL queries. How should we determine if the generated queries are correct? We can't merely compare against the gold SQL queries, since there are many ways to implement a SQL query that answers any given NL query.\n",
    "\n",
    "Instead, we will execute the queries – both the predicted SQL query and the gold SQL query – on an actual database, and verify that the returned responses are the same. For that purpose, we need a SQL database server to use. We'll set one up here, using the [Python `sqlite3` module](https://docs.python.org/3.8/library/sqlite3.html)."
   ],
   "id": "fd104480"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "1de75894"
   },
   "outputs": [],
   "source": [
    "@func_set_timeout(TIMEOUT)\n",
    "def execute_sql(sql):\n",
    "  conn = sqlite3.connect('data/atis_sqlite.db')  # establish the DB based on the downloaded data\n",
    "  c = conn.cursor()                              # build a \"cursor\"\n",
    "  c.execute(sql)\n",
    "  results = list(c.fetchall())\n",
    "  c.close()\n",
    "  conn.close()\n",
    "  return results"
   ],
   "id": "1de75894"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b276ef58"
   },
   "source": [
    "To run a query, we use the cursor's `execute` function, and retrieve the results with `fetchall`. Let's get all the flights that arrive at General Mitchell International – the query `train_sql_1` above. There's a lot, so we'll just print out the first few."
   ],
   "id": "b276ef58"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c5786e3",
    "outputId": "bdd66ccf-8733-4e02-cfd8-468b01cb2300"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Executing: SELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport airport_1 , airport_service airport_service_1 , city city_1 WHERE flight_1.to_airport = airport_1.airport_code AND airport_1.airport_code = 'MKE' AND flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND 1 = 1\n",
      "\n",
      "Result: 534 entries starting with\n",
      "\n",
      "[(107929,), (107930,), (107931,), (107932,), (107933,), (107934,), (107935,), (107936,), (107937,), (107938,)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_ret = execute_sql(train_sql_1)\n",
    "\n",
    "print(f\"\"\"\n",
    "Executing: {train_sql_1}\n",
    "\n",
    "Result: {len(predicted_ret)} entries starting with\n",
    "\n",
    "{predicted_ret[:10]}\n",
    "\"\"\")"
   ],
   "id": "9c5786e3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5033bff7"
   },
   "source": [
    "For your reference, the SQL database we are using has a database schema described at https://github.com/jkkummerfeld/text2sql-data/blob/master/data/atis-schema.csv, and is consistent with the SQL queries provided in the various `.sql` files loaded above."
   ],
   "id": "5033bff7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd23f820"
   },
   "source": [
    "# Rule-based parsing and interpretation of ATIS queries\n",
    "\n",
    "First, you will implement a rule-based semantic parser using a grammar like the one you completed in the third project segment. We've placed an initial grammar in the file `data/grammar`.\n",
    "In addition to the helper functions defined above (`constant`, `first`, etc.), it makes use of some other simple functions. We've included those below, but you can (and almost certainly should) augment this set with others that you define as you build out the full set of augmentations.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: helper_functions\n",
    "-->"
   ],
   "id": "fd23f820"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "c3c8f0f5"
   },
   "outputs": [],
   "source": [
    "def upper(term):\n",
    "  return '\"' + term.upper() + '\"'\n",
    "\n",
    "def weekday(day):\n",
    "  return f\"flight.flight_days IN (SELECT days.days_code FROM days WHERE days.day_name = '{day.upper()}')\"\n",
    "\n",
    "def month_name(month):\n",
    "  return {'JANUARY' : 1,\n",
    "          'FEBRUARY' : 2,\n",
    "          'MARCH' : 3,\n",
    "          'APRIL' : 4,\n",
    "          'MAY' : 5,\n",
    "          'JUNE' : 6,\n",
    "          'JULY' : 7,\n",
    "          'AUGUST' : 8,\n",
    "          'SEPTEMBER' : 9,\n",
    "          'OCTOBER' : 10,\n",
    "          'NOVEMBER' : 11,\n",
    "          'DECEMBER' : 12}[month.upper()]\n",
    "\n",
    "def airports_from_airport_name(airport_name):\n",
    "  return f\"(SELECT airport.airport_code FROM airport WHERE airport.airport_name = {upper(airport_name)})\"\n",
    "\n",
    "def airports_from_city(city):\n",
    "  return f\"\"\"\n",
    "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
    "      (SELECT city.city_code FROM city WHERE city.city_name = {upper(city)}))\n",
    "  \"\"\"\n",
    "\n",
    "def null_condition(*args, **kwargs):\n",
    "  return 1\n",
    "\n",
    "def depart_around(time):\n",
    "  return f\"\"\"\n",
    "    flight.departure_time >= {add_delta(miltime(time), -15).strftime('%H%M')}\n",
    "    AND flight.departure_time <= {add_delta(miltime(time), 15).strftime('%H%M')}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def arrive_around(time):\n",
    "  return f\"\"\"\n",
    "    flight.arrival_time >= {add_delta(miltime(time), -15).strftime('%H%M')}\n",
    "    AND flight.arrival_time <= {add_delta(miltime(time), 15).strftime('%H%M')}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def add_delta(tme, delta):\n",
    "    # transform to a full datetime first\n",
    "    return (datetime.datetime.combine(datetime.date.today(), tme) +\n",
    "            datetime.timedelta(minutes=delta)).time()\n",
    "\n",
    "def miltime(minutes):\n",
    "  return datetime.time(hour=int(minutes/100), minute=(minutes % 100))\n"
   ],
   "id": "c3c8f0f5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef45571a"
   },
   "source": [
    "We can build a parser with the augmented grammar:"
   ],
   "id": "ef45571a"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3ba42b3a"
   },
   "outputs": [],
   "source": [
    "atis_grammar, atis_augmentations = xform.read_augmented_grammar('data/grammar', globals=globals())\n",
    "atis_parser = nltk.parse.BottomUpChartParser(atis_grammar)"
   ],
   "id": "3ba42b3a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4645008"
   },
   "source": [
    "We'll define a function to return a parse tree for a string according to the ATIS grammar (if available)."
   ],
   "id": "f4645008"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4cc626cd"
   },
   "outputs": [],
   "source": [
    "def parse_tree(sentence):\n",
    "  \"\"\"Parse a sentence and return the parse tree, or None if failure.\"\"\"\n",
    "  try:\n",
    "    parses = list(atis_parser.parse(tokenize_nltk(sentence)))\n",
    "    if len(parses) == 0:\n",
    "      return None\n",
    "    else:\n",
    "      return parses[0]\n",
    "  except:\n",
    "    return None"
   ],
   "id": "4cc626cd"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34c97e0e"
   },
   "source": [
    "We can check the overall coverage of this grammar on the training set by using the `parse_tree` function to determine if a parse is available. The grammar that we provide should get about a 44% coverage of the training set."
   ],
   "id": "34c97e0e"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "184f8699",
    "outputId": "53074e32-29d4-4546-92a6-6da2471dfd1c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3651/3651 [00:23<00:00, 152.17it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Parsed 1609 of 3651 (44.07%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check coverage on training set\n",
    "parsed = 0\n",
    "with open(\"data/train_flightid.nl\") as train:\n",
    "  examples = train.readlines()[:]\n",
    "for sentence in tqdm(examples):\n",
    "  if parse_tree(sentence):\n",
    "    parsed += 1\n",
    "  else:\n",
    "    next\n",
    "\n",
    "print(f\"\\nParsed {parsed} of {len(examples)} ({parsed*100/(len(examples)):.2f}%)\")"
   ],
   "id": "184f8699"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67844ce9"
   },
   "source": [
    "## Goal 1: Construct SQL queries from a parse tree and evaluate the results"
   ],
   "id": "67844ce9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee888162"
   },
   "source": [
    "It's time to turn to the first major part of this project segment, implementing a rule-based semantic parsing system to answer flight-ID-type ATIS queries.\n",
    "\n",
    "Recall that in rule-based semantic parsing, each syntactic rule is associated with a semantic composition rule. The grammar we've provided has semantic augmentations for some of the low-level phrases – cities, airports, times, airlines – but not the higher level syntactic types. You'll be adding those.\n",
    "\n",
    "In the ATIS grammar that we provide, as with the earlier toy grammars, the augmentation for a rule with $n$ nonterminals and $m$ terminals on the right-hand side is assumed to be called with $n$ positional arguments (the values for the corresponding children). The `interpret` function you've already defined should therefore work well with this grammar.\n",
    "\n",
    "Let's run through one way that a semantic derivation might proceed, for the sample query \"flights to boston\":"
   ],
   "id": "ee888162"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a684dccb",
    "outputId": "1772279e-adf9-4132-ed5f-09539cb88a3b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['flights', 'to', 'boston']\n",
      "                S                         \n",
      "                |                          \n",
      "            NP_FLIGHT                     \n",
      "                |                          \n",
      "            NOM_FLIGHT                    \n",
      "                |                          \n",
      "             N_FLIGHT                     \n",
      "      __________|_________                 \n",
      "     |                    PP              \n",
      "     |                    |                \n",
      "     |                 PP_PLACE           \n",
      "     |           _________|_________       \n",
      "  N_FLIGHT      |                N_PLACE  \n",
      "     |          |                   |      \n",
      "TERM_FLIGHT  P_PLACE            TERM_PLACE\n",
      "     |          |                   |      \n",
      "  flights       to                boston  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"flights to boston\"\n",
    "print(tokenize_nltk(sample_query))\n",
    "sample_tree = parse_tree(sample_query)\n",
    "sample_tree.pretty_print()"
   ],
   "id": "a684dccb"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a0d63a4"
   },
   "source": [
    "Given a sentence, we first construct its parse tree using the syntactic rules, then compose the corresponding semantic rules bottom-up, until eventually we arrive at the root node with a finished SQL statement. For this query, we will go through what the possible meaning representations for the subconstituents of \"flights to boston\" might be. But this is just one way of doing things; other ways are possible, and you should feel free to experiment.\n",
    "\n",
    "Working from bottom up:\n",
    "\n",
    "1. The `TERM_PLACE` phrase \"boston\" uses the composition function template `constant(airports_from_city(' '.join(_RHS)))`, which will be instantiated as `constant(airports_from_city(' '.join(['boston'])))` (recall that `_RHS` is replaced by the right-hand side of the rule). The meaning of `TERM_PLACE` will be the SQL snippet\n",
    "```\n",
    "SELECT airport_service.airport_code\n",
    "FROM airport_service\n",
    "WHERE airport_service.city_code IN\n",
    "    (SELECT city.city_code\n",
    "     FROM city\n",
    "     WHERE city.city_name = \"BOSTON\")\n",
    "```\n",
    "(This query generates a list of all of the airports in Boston.)\n",
    "\n",
    "2. The `N_PLACE` phrase \"boston\" can have the same meaning as the `TERM_PLACE`.\n",
    "\n",
    "3. The `P_PLACE` phrase \"to\" might be associated with a function that maps a SQL query for a list of airports to a SQL condition that holds of flights that go to one of those airports, i.e., `flight.to_airport IN (...)`.\n",
    "\n",
    "4. The `PP_PLACE` phrase \"to boston\" might apply the `P_PLACE` meaning to the `TERM_PLACE` meaning, thus generating a SQL condition that holds of flights that go to one of the Boston airports:\n",
    "```\n",
    "flight.to_airport IN\n",
    "    (SELECT airport_service.airport_code\n",
    "     FROM airport_service\n",
    "     WHERE airport_service.city_code IN\n",
    "         (SELECT city.city_code\n",
    "          FROM city\n",
    "          WHERE city.city_name = \"BOSTON\"))\n",
    "```\n",
    "\n",
    "5. The `PP` phrase \"to Boston\" can again get its meaning from the `PP_PLACE`.\n",
    "\n",
    "6. The `TERM_FLIGHT` phrase \"flights\" might also return a condition on flights, this time the \"null condition\", represented by the SQL truth value `1`. Ditto for the `N_FLIGHT` phrase \"flights\".\n",
    "\n",
    "7. The `N_FLIGHT` phrase \"flights to boston\" can conjoin the two conditions, yielding the SQL condition\n",
    "```\n",
    "flight.to_airport IN\n",
    "    (SELECT airport_service.airport_code\n",
    "     FROM airport_service\n",
    "     WHERE airport_service.city_code IN\n",
    "         (SELECT city.city_code\n",
    "          FROM city\n",
    "          WHERE city.city_name = \"BOSTON\"))\n",
    "AND 1\n",
    "```\n",
    "which can be inherited by the `NOM_FLIGHT` and `NP_FLIGHT` phrases.\n",
    "\n",
    "8. The `S` phrase \"flights to boston\" can use the condition provided by the `NP_FLIGHT` phrase to select all flights satisfying the condition with a SQL query like\n",
    "```\n",
    "SELECT DISTINCT flight.flight_id\n",
    "FROM flight\n",
    "WHERE flight.to_airport IN\n",
    "        (SELECT airport_service.airport_code\n",
    "         FROM airport_service\n",
    "         WHERE airport_service.city_code IN\n",
    "             (SELECT city.city_code\n",
    "              FROM city\n",
    "              WHERE city.city_name = \"BOSTON\"))\n",
    "      AND 1\n",
    "```\n",
    "\n",
    "This SQL query is then taken to be a representation of the meaning for the NL query \"flights to boston\", and can be executed against the ATIS database to retrieve the requested flights."
   ],
   "id": "1a0d63a4"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b44a5b8"
   },
   "source": [
    "Now, it's your turn to add augmentations to `data/grammar` to make this example work. The augmentations that we have provided for the grammar make use of a set of auxiliary functions that we defined above. You should feel free to add your own auxiliary functions that you make use of in the grammar."
   ],
   "id": "0b44a5b8"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dee1c62",
    "outputId": "c0b311a0-e0be-4377-9ecb-eb967644492c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight.flight_id FROM flight WHERE 1 AND flight.to_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"BOSTON\"))\n",
      "   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: add augmentations to `data/grammar` to make this example work\n",
    "atis_grammar, atis_augmentations = xform.read_augmented_grammar('data/grammar', globals=globals())\n",
    "atis_parser = nltk.parse.BottomUpChartParser(atis_grammar)\n",
    "predicted_sql = interpret(sample_tree, atis_augmentations)\n",
    "print(\"Predicted SQL:\\n\\n\", predicted_sql, \"\\n\")"
   ],
   "id": "4dee1c62"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51d5a690"
   },
   "source": [
    "#### Verification on some examples\n",
    "\n",
    "With a rule-based semantic parsing system, we can generate SQL queries given questions, and then execute those queries on a SQL database to answer the given questions. To evaluate the performance of the system, we compare the returned results against the results of executing the ground truth queries.\n",
    "\n",
    "We provide a function `verify` to compare the results from our generated SQL to the ground truth SQL. It should be useful for testing individual queries."
   ],
   "id": "51d5a690"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "497bc442"
   },
   "outputs": [],
   "source": [
    "def verify(predicted_sql, gold_sql, silent=True):\n",
    "  \"\"\"\n",
    "  Compare the correctness of the generated SQL by executing on the\n",
    "  ATIS database and comparing the returned results.\n",
    "  Arguments:\n",
    "      predicted_sql: the predicted SQL query\n",
    "      gold_sql: the reference SQL query to compare against\n",
    "      silent: print outputs or not\n",
    "  Returns: True if the returned results are the same, otherwise False\n",
    "  \"\"\"\n",
    "  # Execute predicted SQL\n",
    "  try:\n",
    "    predicted_result = execute_sql(predicted_sql)\n",
    "  except BaseException as e:\n",
    "    if not silent:\n",
    "      print(f\"predicted sql exec failed: {e}\")\n",
    "    return False\n",
    "  if not silent:\n",
    "    print(\"Predicted DB result:\\n\\n\", predicted_result[:10], \"\\n\")\n",
    "\n",
    "  # Execute gold SQL\n",
    "  try:\n",
    "    gold_result = execute_sql(gold_sql)\n",
    "  except BaseException as e:\n",
    "    if not silent:\n",
    "      print(f\"gold sql exec failed: {e}\")\n",
    "    return False\n",
    "  if not silent:\n",
    "    print(\"Gold DB result:\\n\\n\", gold_result[:10], \"\\n\")\n",
    "\n",
    "  # Verify correctness\n",
    "  if gold_result == predicted_result:\n",
    "    return True"
   ],
   "id": "497bc442"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caf469c0"
   },
   "source": [
    "Let's try this methodology on a simple example: \"flights from phoenix to milwaukee\". we provide it along with the gold SQL query."
   ],
   "id": "caf469c0"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "57db3e51"
   },
   "outputs": [],
   "source": [
    "def rule_based_trial(sentence, gold_sql):\n",
    "  print(\"Sentence: \", sentence, \"\\n\")\n",
    "  tree = parse_tree(sentence)\n",
    "  print(\"Parse:\\n\\n\")\n",
    "  tree.pretty_print()\n",
    "\n",
    "  predicted_sql = interpret(tree, atis_augmentations)\n",
    "  print(\"Predicted SQL:\\n\\n\", predicted_sql, \"\\n\")\n",
    "\n",
    "  if verify(predicted_sql, gold_sql, silent=False):\n",
    "    print ('Correct!')\n",
    "  else:\n",
    "    print ('Incorrect!')"
   ],
   "id": "57db3e51"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "e274f3e3"
   },
   "outputs": [],
   "source": [
    "# Run this cell to reload augmentations after you make changes to `data/grammar`\n",
    "atis_grammar, atis_augmentations = xform.read_augmented_grammar('data/grammar', globals=globals())\n",
    "atis_parser = nltk.parse.BottomUpChartParser(atis_grammar)"
   ],
   "id": "e274f3e3"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f9d2e96",
    "outputId": "321dd3c3-af60-4b36-b052-896638a86df7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  flights from phoenix to milwaukee \n",
      "\n",
      "Parse:\n",
      "\n",
      "\n",
      "                                  S                                 \n",
      "                                  |                                  \n",
      "                              NP_FLIGHT                             \n",
      "                                  |                                  \n",
      "                              NOM_FLIGHT                            \n",
      "                                  |                                  \n",
      "                               N_FLIGHT                             \n",
      "                __________________|_________________                 \n",
      "            N_FLIGHT                                |               \n",
      "      _________|________                            |                \n",
      "     |                  PP                          PP              \n",
      "     |                  |                           |                \n",
      "     |               PP_PLACE                    PP_PLACE           \n",
      "     |          ________|_________           _______|_________       \n",
      "  N_FLIGHT     |               N_PLACE      |              N_PLACE  \n",
      "     |         |                  |         |                 |      \n",
      "TERM_FLIGHT P_PLACE           TERM_PLACE P_PLACE          TERM_PLACE\n",
      "     |         |                  |         |                 |      \n",
      "  flights     from             phoenix      to            milwaukee \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight.flight_id FROM flight WHERE 1 AND flight.from_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"PHOENIX\"))\n",
      "   AND flight.to_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"MILWAUKEE\"))\n",
      "   \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(108086,), (108087,), (301763,), (301764,), (301765,), (301766,), (302323,), (304881,), (310619,), (310620,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(108086,), (108087,), (301763,), (301764,), (301765,), (301766,), (302323,), (304881,), (310619,), (310620,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "#TODO: add augmentations to `data/grammar` to make this example work\n",
    "# Example 1\n",
    "example_1 = 'flights from phoenix to milwaukee'\n",
    "gold_sql_1 = \"\"\"\n",
    "  SELECT DISTINCT flight_1.flight_id\n",
    "  FROM flight flight_1 ,\n",
    "       airport_service airport_service_1 ,\n",
    "       city city_1 ,\n",
    "       airport_service airport_service_2 ,\n",
    "       city city_2\n",
    "  WHERE flight_1.from_airport = airport_service_1.airport_code\n",
    "        AND airport_service_1.city_code = city_1.city_code\n",
    "        AND city_1.city_name = 'PHOENIX'\n",
    "        AND flight_1.to_airport = airport_service_2.airport_code\n",
    "        AND airport_service_2.city_code = city_2.city_code\n",
    "        AND city_2.city_name = 'MILWAUKEE'\n",
    "  \"\"\"\n",
    "\n",
    "rule_based_trial(example_1, gold_sql_1)"
   ],
   "id": "0f9d2e96"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9e90898"
   },
   "source": [
    "To make development faster, we recommend starting with a few examples before running the full evaluation script. We've taken some examples from the ATIS dataset including the gold SQL queries that they provided. Of course, yours (and those of the project segment solution set) may differ."
   ],
   "id": "f9e90898"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2af3beb4",
    "outputId": "3ad27a66-a2be-49cb-a55c-a6fadcc00c77"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  i would like a united flight \n",
      "\n",
      "Parse:\n",
      "\n",
      "\n",
      "                                                 S                                                                      \n",
      "                                     ____________|____________________________________________________                   \n",
      "                                    |                                                             NP_FLIGHT             \n",
      "                                    |                                                                 |                  \n",
      "                                PREIGNORE                                                         NOM_FLIGHT            \n",
      "        ____________________________|____________                                          ___________|___________       \n",
      "       |                                     PREIGNORE                                   ADJ                      |     \n",
      "       |                _________________________|____________                            |                       |      \n",
      "       |               |                                  PREIGNORE                  ADJ_AIRLINE              NOM_FLIGHT\n",
      "       |               |                          ____________|____________               |                       |      \n",
      "       |               |                         |                     PREIGNORE     TERM_AIRLINE              N_FLIGHT \n",
      "       |               |                         |                         |              |                       |      \n",
      "PREIGNORESYMBOL PREIGNORESYMBOL           PREIGNORESYMBOL           PREIGNORESYMBOL TERM_AIRBRAND            TERM_FLIGHT\n",
      "       |               |                         |                         |              |                       |      \n",
      "       i             would                      like                       a            united                  flight  \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight.flight_id FROM flight WHERE flight.airline_code = \"UA\" AND 1 \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(100094,), (100099,), (100145,), (100158,), (100164,), (100167,), (100169,), (100203,), (100204,), (100296,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(100094,), (100099,), (100145,), (100158,), (100164,), (100167,), (100169,), (100203,), (100204,), (100296,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "#TODO: add augmentations to `data/grammar` to make this example work\n",
    "# Example 2\n",
    "example_2 = 'i would like a united flight'\n",
    "gold_sql_2 = \"\"\"\n",
    "  SELECT DISTINCT flight_1.flight_id\n",
    "  FROM flight flight_1\n",
    "  WHERE flight_1.airline_code = 'UA'\n",
    "  \"\"\"\n",
    "\n",
    "rule_based_trial(example_2, gold_sql_2)"
   ],
   "id": "2af3beb4"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03c0406b",
    "outputId": "28be03ef-d483-4ee9-9491-2511ddd1d956"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  i would like a flight between boston and dallas \n",
      "\n",
      "Parse:\n",
      "\n",
      "\n",
      "                                                                                         S                                               \n",
      "                                     ____________________________________________________|__________                                      \n",
      "                                    |                                                           NP_FLIGHT                                \n",
      "                                    |                                                               |                                     \n",
      "                                    |                                                           NOM_FLIGHT                               \n",
      "                                    |                                                               |                                     \n",
      "                                PREIGNORE                                                        N_FLIGHT                                \n",
      "        ____________________________|____________                                         __________|_________                            \n",
      "       |                                     PREIGNORE                                   |                    PP                         \n",
      "       |                _________________________|____________                           |                    |                           \n",
      "       |               |                                  PREIGNORE                      |                 PP_PLACE                      \n",
      "       |               |                          ____________|____________              |           _________|____________________       \n",
      "       |               |                         |                     PREIGNORE      N_FLIGHT      |         |      N_PLACE    N_PLACE  \n",
      "       |               |                         |                         |             |          |         |         |          |      \n",
      "PREIGNORESYMBOL PREIGNORESYMBOL           PREIGNORESYMBOL           PREIGNORESYMBOL TERM_FLIGHT     |         |     TERM_PLACE TERM_PLACE\n",
      "       |               |                         |                         |             |          |         |         |          |      \n",
      "       i             would                      like                       a           flight    between     and      boston     dallas  \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight.flight_id FROM flight WHERE 1 AND flight.from_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"BOSTON\"))\n",
      "   AND flight.to_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"DALLAS\"))\n",
      "   \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(103171,), (103172,), (103173,), (103174,), (103175,), (103176,), (103177,), (103178,), (103179,), (103180,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(103171,), (103172,), (103173,), (103174,), (103175,), (103176,), (103177,), (103178,), (103179,), (103180,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "#TODO: add augmentations to `data/grammar` to make this example work\n",
    "# Example 3\n",
    "example_3 = 'i would like a flight between boston and dallas'\n",
    "gold_sql_3 = \"\"\"\n",
    "  SELECT DISTINCT flight_1.flight_id\n",
    "  FROM flight flight_1 ,\n",
    "       airport_service airport_service_1 ,\n",
    "       city city_1 ,\n",
    "       airport_service airport_service_2 ,\n",
    "       city city_2\n",
    "  WHERE flight_1.from_airport = airport_service_1.airport_code\n",
    "        AND airport_service_1.city_code = city_1.city_code\n",
    "        AND city_1.city_name = 'BOSTON'\n",
    "        AND flight_1.to_airport = airport_service_2.airport_code\n",
    "        AND airport_service_2.city_code = city_2.city_code\n",
    "        AND city_2.city_name = 'DALLAS'\n",
    "  \"\"\"\n",
    "\n",
    "# Note that the parse tree might appear wrong: instead of\n",
    "# `PP_PLACE -> 'between' N_PLACE 'and' N_PLACE`, the tree appears to be\n",
    "# `PP_PLACE -> 'between' 'and' N_PLACE N_PLACE`. But it's only a visualization\n",
    "# error of tree.pretty_print() and you should assume that the production is\n",
    "# `PP_PLACE -> 'between' N_PLACE 'and' N_PLACE` (you can verify by printing out\n",
    "# all productions).\n",
    "rule_based_trial(example_3, gold_sql_3)"
   ],
   "id": "03c0406b"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18f27e89",
    "outputId": "323d9a88-27e4-4af5-90d8-6e2fa87c8c9f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  show me the united flights from denver to baltimore \n",
      "\n",
      "Parse:\n",
      "\n",
      "\n",
      "                                                                             S                                                                \n",
      "                        _____________________________________________________|__________                                                       \n",
      "                       |                                                            NP_FLIGHT                                                 \n",
      "                       |                                                                |                                                      \n",
      "                       |                                                            NOM_FLIGHT                                                \n",
      "                       |                                         _______________________|___________________                                   \n",
      "                       |                                        |                                       NOM_FLIGHT                            \n",
      "                       |                                        |                                           |                                  \n",
      "                       |                                        |                                        N_FLIGHT                             \n",
      "                       |                                        |                        ___________________|_________________                 \n",
      "                       |                                        |                    N_FLIGHT                                 |               \n",
      "                       |                                        |             __________|_________                            |                \n",
      "                   PREIGNORE                                   ADJ           |                    PP                          PP              \n",
      "        _______________|____________                            |            |                    |                           |                \n",
      "       |                        PREIGNORE                  ADJ_AIRLINE       |                 PP_PLACE                    PP_PLACE           \n",
      "       |                ____________|____________               |            |           _________|_________           _______|_________       \n",
      "       |               |                     PREIGNORE     TERM_AIRLINE   N_FLIGHT      |                N_PLACE      |              N_PLACE  \n",
      "       |               |                         |              |            |          |                   |         |                 |      \n",
      "PREIGNORESYMBOL PREIGNORESYMBOL           PREIGNORESYMBOL TERM_AIRBRAND TERM_FLIGHT  P_PLACE            TERM_PLACE P_PLACE          TERM_PLACE\n",
      "       |               |                         |              |            |          |                   |         |                 |      \n",
      "      show             me                       the           united      flights      from               denver      to            baltimore \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight.flight_id FROM flight WHERE flight.airline_code = \"UA\" AND 1 AND flight.from_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"DENVER\"))\n",
      "   AND flight.to_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"BALTIMORE\"))\n",
      "   \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(101231,), (101233,), (305983,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(101231,), (101233,), (305983,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "#TODO: add augmentations to `data/grammar` to make this example work\n",
    "# Example 4\n",
    "example_4 = 'show me the united flights from denver to baltimore'\n",
    "gold_sql_4 = \"\"\"\n",
    "  SELECT DISTINCT flight_1.flight_id\n",
    "  FROM flight flight_1 ,\n",
    "       airport_service airport_service_1 ,\n",
    "       city city_1 ,\n",
    "       airport_service airport_service_2 ,\n",
    "       city city_2\n",
    "  WHERE flight_1.airline_code = 'UA'\n",
    "        AND ( flight_1.from_airport = airport_service_1.airport_code\n",
    "              AND airport_service_1.city_code = city_1.city_code\n",
    "              AND city_1.city_name = 'DENVER'\n",
    "              AND flight_1.to_airport = airport_service_2.airport_code\n",
    "              AND airport_service_2.city_code = city_2.city_code\n",
    "              AND city_2.city_name = 'BALTIMORE' )\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "rule_based_trial(example_4, gold_sql_4)"
   ],
   "id": "18f27e89"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4e06de0",
    "outputId": "14cc4e3a-58a9-4a2d-f18a-80a0626d2ff1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  show flights from cleveland to miami that arrive before 4pm \n",
      "\n",
      "Parse:\n",
      "\n",
      "\n",
      "                               S                                                                                                         \n",
      "        _______________________|______________________________________________                                                            \n",
      "       |                                                                  NP_FLIGHT                                                      \n",
      "       |                                                                      |                                                           \n",
      "       |                                                                  NOM_FLIGHT                                                     \n",
      "       |                                                                      |                                                           \n",
      "       |                                                                   N_FLIGHT                                                      \n",
      "       |                                           ___________________________|_____________________                                      \n",
      "       |                                       N_FLIGHT                                             |                                    \n",
      "       |                        __________________|_________________                                |                                     \n",
      "       |                    N_FLIGHT                                |                               PP                                   \n",
      "       |              _________|________                            |                               |                                     \n",
      "       |             |                  PP                          PP                           PP_TIME                                 \n",
      "       |             |                  |                           |                        _______|__________________                   \n",
      "       |             |               PP_PLACE                    PP_PLACE                   |                       NP_TIME              \n",
      "       |             |          ________|_________           _______|_________              |                          |                  \n",
      "   PREIGNORE      N_FLIGHT     |               N_PLACE      |              N_PLACE          |                      TERM_TIME             \n",
      "       |             |         |                  |         |                 |             |                 _________|__________        \n",
      "PREIGNORESYMBOL TERM_FLIGHT P_PLACE           TERM_PLACE P_PLACE          TERM_PLACE      P_TIME         TERM_TIME           TERM_TIMEMOD\n",
      "       |             |         |                  |         |                 |        _____|_______         |                    |       \n",
      "      show        flights     from            cleveland     to              miami    that arrive  before     4                    pm     \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight.flight_id FROM flight WHERE 1 AND flight.from_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"CLEVELAND\"))\n",
      "   AND flight.to_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"MIAMI\"))\n",
      "   AND flight.arrival_time < 1600 \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(107698,), (301117,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(107698,), (301117,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "#TODO: add augmentations to `data/grammar` to make this example work\n",
    "# Example 5\n",
    "example_5 = 'show flights from cleveland to miami that arrive before 4pm'\n",
    "gold_sql_5 = \"\"\"\n",
    "  SELECT DISTINCT flight_1.flight_id\n",
    "  FROM flight flight_1 ,\n",
    "       airport_service airport_service_1 ,\n",
    "       city city_1 ,\n",
    "       airport_service airport_service_2 ,\n",
    "       city city_2\n",
    "  WHERE flight_1.from_airport = airport_service_1.airport_code\n",
    "        AND airport_service_1.city_code = city_1.city_code\n",
    "        AND city_1.city_name = 'CLEVELAND'\n",
    "        AND ( flight_1.to_airport = airport_service_2.airport_code\n",
    "              AND airport_service_2.city_code = city_2.city_code\n",
    "              AND city_2.city_name = 'MIAMI'\n",
    "              AND flight_1.arrival_time < 1600 )\n",
    "  \"\"\"\n",
    "\n",
    "rule_based_trial(example_5, gold_sql_5)"
   ],
   "id": "a4e06de0"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aa401d5",
    "outputId": "6da29276-f848-4929-dabf-9ea6b3287b2f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  okay how about a flight on sunday from tampa to charlotte \n",
      "\n",
      "Parse:\n",
      "\n",
      "\n",
      "                                                                                                   S                                                                                    \n",
      "                                     ______________________________________________________________|______________________________                                                       \n",
      "                                    |                                                                                         NP_FLIGHT                                                 \n",
      "                                    |                                                                                             |                                                      \n",
      "                                    |                                                                                         NOM_FLIGHT                                                \n",
      "                                    |                                                                                             |                                                      \n",
      "                                    |                                                                                          N_FLIGHT                                                 \n",
      "                                    |                                                                                  ___________|_____________________________________                 \n",
      "                                    |                                                                              N_FLIGHT                                             |               \n",
      "                                    |                                                               __________________|_____________________                            |                \n",
      "                                PREIGNORE                                                       N_FLIGHT                                    |                           |               \n",
      "        ____________________________|____________                                         _________|________                                |                           |                \n",
      "       |                                     PREIGNORE                                   |                  PP                              PP                          PP              \n",
      "       |                _________________________|____________                           |                  |                               |                           |                \n",
      "       |               |                                  PREIGNORE                      |               PP_DATE                         PP_PLACE                    PP_PLACE           \n",
      "       |               |                          ____________|____________              |          ________|_________             _________|_________           _______|_________       \n",
      "       |               |                         |                     PREIGNORE      N_FLIGHT     |               NP_DATE        |                N_PLACE      |              N_PLACE  \n",
      "       |               |                         |                         |             |         |                  |           |                   |         |                 |      \n",
      "PREIGNORESYMBOL PREIGNORESYMBOL           PREIGNORESYMBOL           PREIGNORESYMBOL TERM_FLIGHT  P_DATE          TERM_WEEKDAY  P_PLACE            TERM_PLACE P_PLACE          TERM_PLACE\n",
      "       |               |                         |                         |             |         |                  |           |                   |         |                 |      \n",
      "      okay            how                      about                       a           flight      on               sunday       from               tampa       to            charlotte \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight.flight_id FROM flight WHERE 1 AND flight.flight_days IN (SELECT days.days_code FROM days WHERE days.day_name = 'SUNDAY') AND flight.from_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"TAMPA\"))\n",
      "   AND flight.to_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"CHARLOTTE\"))\n",
      "   \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(101860,), (101861,), (101862,), (101863,), (101864,), (101865,), (305231,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(101860,), (101861,), (101862,), (101863,), (101864,), (101865,), (305231,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "#TODO: add augmentations to `data/grammar` to make this example work\n",
    "# Example 6\n",
    "example_6 = 'okay how about a flight on sunday from tampa to charlotte'\n",
    "gold_sql_6 = \"\"\"\n",
    "  SELECT DISTINCT flight_1.flight_id\n",
    "  FROM flight flight_1 ,\n",
    "       airport_service airport_service_1 ,\n",
    "       city city_1 ,\n",
    "       airport_service airport_service_2 ,\n",
    "       city city_2 ,\n",
    "       days days_1 ,\n",
    "       date_day date_day_1\n",
    "  WHERE flight_1.from_airport = airport_service_1.airport_code\n",
    "        AND airport_service_1.city_code = city_1.city_code\n",
    "        AND city_1.city_name = 'TAMPA'\n",
    "        AND ( flight_1.to_airport = airport_service_2.airport_code\n",
    "              AND airport_service_2.city_code = city_2.city_code\n",
    "              AND city_2.city_name = 'CHARLOTTE'\n",
    "              AND flight_1.flight_days = days_1.days_code\n",
    "              AND days_1.day_name = date_day_1.day_name\n",
    "              AND date_day_1.year = 1991\n",
    "              AND date_day_1.month_number = 8\n",
    "              AND date_day_1.day_number = 27 )\n",
    "  \"\"\"\n",
    "\n",
    "# You might notice that the gold answer above used the exact date, which is\n",
    "# not easily implementable. A more implementable way (generated by the project\n",
    "# segment 4 solution code) is:\n",
    "gold_sql_6b = \"\"\"\n",
    "  SELECT DISTINCT flight.flight_id\n",
    "  FROM flight\n",
    "  WHERE ((((1\n",
    "            AND flight.flight_days IN (SELECT days.days_code\n",
    "                                       FROM days\n",
    "                                       WHERE days.day_name = 'SUNDAY')\n",
    "            )\n",
    "           AND flight.from_airport IN (SELECT airport_service.airport_code\n",
    "                                       FROM airport_service\n",
    "                                       WHERE airport_service.city_code IN (SELECT city.city_code\n",
    "                                                                           FROM city\n",
    "                                                                           WHERE city.city_name = \"TAMPA\")))\n",
    "          AND flight.to_airport IN (SELECT airport_service.airport_code\n",
    "                                    FROM airport_service\n",
    "                                    WHERE airport_service.city_code IN (SELECT city.city_code\n",
    "                                                                        FROM city\n",
    "                                                                        WHERE city.city_name = \"CHARLOTTE\"))))\n",
    "  \"\"\"\n",
    "\n",
    "rule_based_trial(example_6, gold_sql_6b)"
   ],
   "id": "4aa401d5"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10256866",
    "outputId": "cd10056c-0427-45e3-a6dc-0f657d0e43b1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  list all flights going from boston to atlanta that leaves before 7 am on thursday \n",
      "\n",
      "Parse:\n",
      "\n",
      "\n",
      "                                  S                                                                                                                                                  \n",
      "        __________________________|_____________________________________________________                                                                                              \n",
      "       |                                                                            NP_FLIGHT                                                                                        \n",
      "       |          ______________________________________________________________________|__________                                                                                   \n",
      "       |         |                                                                             NOM_FLIGHT                                                                            \n",
      "       |         |                                                                                 |                                                                                  \n",
      "       |         |                                                                              N_FLIGHT                                                                             \n",
      "       |         |                                                                       __________|________________________________________________________________                  \n",
      "       |         |                                                                   N_FLIGHT                                                                       |                \n",
      "       |         |                                 _____________________________________|___________________________                                                |                 \n",
      "       |         |                             N_FLIGHT                                                             |                                               |                \n",
      "       |         |                        ________|___________________________                                      |                                               |                 \n",
      "       |         |                    N_FLIGHT                                |                                     PP                                              |                \n",
      "       |         |        _______________|________                            |                                     |                                               |                 \n",
      "       |         |       |                        PP                          PP                                 PP_TIME                                            PP               \n",
      "       |         |       |                        |                           |                              _______|__________________                             |                 \n",
      "       |         |       |                     PP_PLACE                    PP_PLACE                         |                       NP_TIME                      PP_DATE             \n",
      "       |         |       |                ________|_________           _______|_________                    |                          |                     _______|_________        \n",
      "   PREIGNORE     |    N_FLIGHT           |               N_PLACE      |              N_PLACE                |                      TERM_TIME                |              NP_DATE   \n",
      "       |         |       |               |                  |         |                 |                   |                 _________|__________          |                 |       \n",
      "PREIGNORESYMBOL DET TERM_FLIGHT       P_PLACE           TERM_PLACE P_PLACE          TERM_PLACE            P_TIME         TERM_TIME           TERM_TIMEMOD P_DATE         TERM_WEEKDAY\n",
      "       |         |       |         ______|________          |         |                 |           ________|_______         |                    |         |                 |       \n",
      "      list      all   flights   going            from     boston      to             atlanta      that    leaves  before     7                    am        on             thursday  \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight.flight_id FROM flight WHERE 1 AND flight.from_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"BOSTON\"))\n",
      "   AND flight.to_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"ATLANTA\"))\n",
      "   AND flight.departure_time < 700 AND flight.flight_days IN (SELECT days.days_code FROM days WHERE days.day_name = 'THURSDAY') \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(100014,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(100014,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "#TODO: add augmentations to `data/grammar` to make this example work\n",
    "# Example 7\n",
    "example_7 = 'list all flights going from boston to atlanta that leaves before 7 am on thursday'\n",
    "gold_sql_7 = \"\"\"\n",
    "  SELECT DISTINCT flight_1.flight_id\n",
    "  FROM flight flight_1 ,\n",
    "       airport_service airport_service_1 ,\n",
    "       city city_1 ,\n",
    "       airport_service airport_service_2 ,\n",
    "       city city_2 ,\n",
    "       days days_1 ,\n",
    "       date_day date_day_1\n",
    "  WHERE flight_1.from_airport = airport_service_1.airport_code\n",
    "        AND airport_service_1.city_code = city_1.city_code\n",
    "        AND city_1.city_name = 'BOSTON'\n",
    "        AND ( flight_1.to_airport = airport_service_2.airport_code\n",
    "              AND airport_service_2.city_code = city_2.city_code\n",
    "              AND city_2.city_name = 'ATLANTA'\n",
    "              AND ( flight_1.flight_days = days_1.days_code\n",
    "                    AND days_1.day_name = date_day_1.day_name\n",
    "                    AND date_day_1.year = 1991\n",
    "                    AND date_day_1.month_number = 5\n",
    "                    AND date_day_1.day_number = 24\n",
    "                    AND flight_1.departure_time < 700 ) )\n",
    "  \"\"\"\n",
    "\n",
    "# Again, the gold answer above used the exact date, as opposed to the\n",
    "# following approach:\n",
    "gold_sql_7b = \"\"\"\n",
    "  SELECT DISTINCT flight.flight_id\n",
    "  FROM flight\n",
    "  WHERE ((1\n",
    "          AND ((((1\n",
    "                  AND flight.from_airport IN (SELECT airport_service.airport_code\n",
    "                                              FROM airport_service\n",
    "                                              WHERE airport_service.city_code IN (SELECT city.city_code\n",
    "                                                                                  FROM city\n",
    "                                                                                  WHERE city.city_name = \"BOSTON\")))\n",
    "                 AND flight.to_airport IN (SELECT airport_service.airport_code\n",
    "                                           FROM airport_service\n",
    "                                           WHERE airport_service.city_code IN (SELECT city.city_code\n",
    "                                                                               FROM city\n",
    "                                                                               WHERE city.city_name = \"ATLANTA\")))\n",
    "                AND flight.departure_time <= 0700)\n",
    "               AND flight.flight_days IN (SELECT days.days_code\n",
    "                                          FROM days\n",
    "                                          WHERE days.day_name = 'THURSDAY'))))\n",
    "  \"\"\"\n",
    "\n",
    "rule_based_trial(example_7, gold_sql_7b)"
   ],
   "id": "10256866"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4a869db",
    "outputId": "e150d258-26d6-445c-8c57-2eb3bb35a93f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  list the flights from dallas to san francisco on american airlines \n",
      "\n",
      "Parse:\n",
      "\n",
      "\n",
      "                                                                                      S                                                                                         \n",
      "                     _________________________________________________________________|________                                                                                  \n",
      "                    |                                                                      NP_FLIGHT                                                                            \n",
      "                    |                                                                          |                                                                                 \n",
      "                    |                                                                      NOM_FLIGHT                                                                           \n",
      "                    |                                                                          |                                                                                 \n",
      "                    |                                                                       N_FLIGHT                                                                            \n",
      "                    |                                                        __________________|___________________________________________                                      \n",
      "                    |                                                    N_FLIGHT                                                          |                                    \n",
      "                    |                                     __________________|__________________                                            |                                     \n",
      "                    |                                 N_FLIGHT                                 |                                           |                                    \n",
      "                    |                           _________|________                             |                                           |                                     \n",
      "                    |                          |                  PP                           PP                                          PP                                   \n",
      "                    |                          |                  |                            |                                           |                                     \n",
      "                PREIGNORE                      |               PP_PLACE                     PP_PLACE                                   PP_AIRLINE                               \n",
      "        ____________|____________              |          ________|_________           ________|__________                      ___________|____________                         \n",
      "       |                     PREIGNORE      N_FLIGHT     |               N_PLACE      |                N_PLACE                 |                   TERM_AIRLINE                 \n",
      "       |                         |             |         |                  |         |                   |                    |            ____________|______________          \n",
      "PREIGNORESYMBOL           PREIGNORESYMBOL TERM_FLIGHT P_PLACE           TERM_PLACE P_PLACE            TERM_PLACE           P_AIRLINE TERM_AIRBRAND              TERM_AIRBRANDTYP\n",
      "       |                         |             |         |                  |         |                   |                    |           |                           E        \n",
      "       |                         |             |         |                  |         |         __________|__________          |           |                           |         \n",
      "      list                      the         flights     from              dallas      to      san                francisco     on       american                    airlines    \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight.flight_id FROM flight WHERE 1 AND flight.from_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"DALLAS\"))\n",
      "   AND flight.to_airport IN \n",
      "    (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n",
      "      (SELECT city.city_code FROM city WHERE city.city_name = \"SAN FRANCISCO\"))\n",
      "   AND flight.airline_code = \"AA\" \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(108452,), (108454,), (108456,), (111083,), (111085,), (111086,), (111090,), (111091,), (111092,), (111094,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(108452,), (108454,), (108456,), (111083,), (111085,), (111086,), (111090,), (111091,), (111092,), (111094,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "#TODO: add augmentations to `data/grammar` to make this example work\n",
    "# Example 8\n",
    "example_8 = 'list the flights from dallas to san francisco on american airlines'\n",
    "gold_sql_8 = \"\"\"\n",
    "  SELECT DISTINCT flight_1.flight_id\n",
    "  FROM flight flight_1 ,\n",
    "       airport_service airport_service_1 ,\n",
    "       city city_1 ,\n",
    "       airport_service airport_service_2 ,\n",
    "       city city_2\n",
    "  WHERE flight_1.airline_code = 'AA'\n",
    "        AND ( flight_1.from_airport = airport_service_1.airport_code\n",
    "              AND airport_service_1.city_code = city_1.city_code\n",
    "              AND city_1.city_name = 'DALLAS'\n",
    "              AND flight_1.to_airport = airport_service_2.airport_code\n",
    "              AND airport_service_2.city_code = city_2.city_code\n",
    "              AND city_2.city_name = 'SAN FRANCISCO' )\n",
    "  \"\"\"\n",
    "\n",
    "rule_based_trial(example_8, gold_sql_8)"
   ],
   "id": "b4a869db"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aedfdaee"
   },
   "source": [
    "### Systematic evaluation on a test set\n",
    "\n",
    "We can perform a more systematic evaluation by checking the accuracy of the queries on an entire test set for which we have gold queries. The `evaluate` function below does just this, calculating precision, recall, and F1 metrics for the test set. It takes as argument a \"predictor\" function, which maps token sequences to predicted SQL queries. We've provided a predictor function for the rule-based model in the next cell (and a predictor for the seq2seq system below when we get to that system).\n",
    "\n",
    "The rule-based system does not generate predictions for all queries; many queries won't parse. The precision and recall metrics take this into account in measuring the efficacy of the method. The recall metric captures what proportion of _all of the test examples_ for which the system generates a correct query. The precision metric captures what proportion of _all of the test examples for which a prediction is generated_ for which the system generates a correct query. (Recall that F1 is just the geometric mean of precision and recall.)\n",
    "\n",
    "Once you've made some progress on adding augmentations to the grammar, you can evaluate your progress by seeing if the precision and recall have improved. For reference, the solution code achieves precision of about 66% and recall of about 28% for an F1 of 39%."
   ],
   "id": "aedfdaee"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "25de63bb"
   },
   "outputs": [],
   "source": [
    "def evaluate(predictor, dataset, num_examples=0, silent=True):\n",
    "  \"\"\"Evaluate accuracy of `predictor` by executing predictions on a\n",
    "  SQL database and comparing returned results against those of gold queries.\n",
    "\n",
    "  Arguments:\n",
    "      predictor:    a function that maps a token sequence\n",
    "                    to a predicted SQL query string\n",
    "      dataset:      the dataset of token sequences and gold SQL queries\n",
    "      num_examples: number of examples from `dataset` to use; all of\n",
    "                    them if 0\n",
    "      silent: if set to False, will print out logs\n",
    "  Returns: precision, recall, and F1 score\n",
    "  \"\"\"\n",
    "  # Prepare to count results\n",
    "  if num_examples <= 0:\n",
    "    num_examples = len(dataset)\n",
    "  example_count = 0\n",
    "  predicted_count = 0\n",
    "  correct = 0\n",
    "  incorrect = 0\n",
    "\n",
    "  # Process the examples from the dataset\n",
    "  for _, example in tqdm(zip(range(num_examples), dataset)):\n",
    "    example_count += 1\n",
    "    # obtain query SQL\n",
    "    predicted_sql = predictor(example['src'])\n",
    "    if predicted_sql == None:\n",
    "      continue\n",
    "    predicted_count += 1\n",
    "    # obtain gold SQL\n",
    "    gold_sql = example['tgt']\n",
    "\n",
    "    # check that they're compatible\n",
    "    if verify(predicted_sql, gold_sql):\n",
    "      correct += 1\n",
    "    else:\n",
    "      incorrect += 1\n",
    "\n",
    "  # Compute and return precision, recall, F1\n",
    "  precision = correct / predicted_count if predicted_count > 0 else 0\n",
    "  recall = correct / example_count\n",
    "  f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "  return precision, recall, f1"
   ],
   "id": "25de63bb"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "e33dfba3"
   },
   "outputs": [],
   "source": [
    "def rule_based_predictor(query):\n",
    "  tree = parse_tree(query)\n",
    "  if tree is None:\n",
    "    return None\n",
    "  try:\n",
    "    predicted_sql = interpret(tree, atis_augmentations)\n",
    "  except Exception as err:\n",
    "    return None\n",
    "  return predicted_sql"
   ],
   "id": "e33dfba3"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2ffe5c7",
    "outputId": "fef0d787-673c-4cce-fa14-53cea3d36e02"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "332it [00:01, 221.50it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "precision: 0.67\n",
      "recall:    0.28\n",
      "F1:        0.39\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = evaluate(rule_based_predictor, test_data, num_examples=0)\n",
    "print(f\"precision: {precision:3.2f}\")\n",
    "print(f\"recall:    {recall:3.2f}\")\n",
    "print(f\"F1:        {f1:3.2f}\")"
   ],
   "id": "e2ffe5c7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa1e29bf"
   },
   "source": [
    "# End-to-End Seq2Seq Model"
   ],
   "id": "fa1e29bf"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3040c18c"
   },
   "source": [
    "In this part, you will implement a seq2seq model **with attention mechanism** to directly learn the translation from NL query to SQL. You might find labs 4-4 and 4-5 particularly helpful, as the primary difference here is that we are using a different dataset.\n",
    "\n",
    "**Note:** We recommend using GPUs to train the model in this part (one way to get GPUs is to use [Google Colab](https://colab.research.google.com) and clicking Menu -> Runtime -> Change runtime type -> GPU), as we need to use a very large model to solve the task well. For development we recommend starting with a smaller model and training for only 1 epoch."
   ],
   "id": "3040c18c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70cc14a7"
   },
   "source": [
    "## Goal 2: Implement a seq2seq model (with attention)"
   ],
   "id": "70cc14a7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edce39ff"
   },
   "source": [
    "In lab 4-5, you implemented a neural encoder-decoder model with attention. That model was used to convert English number phrases to numbers, but one of the biggest advantages of neural models is that we can easily apply them to different tasks (such as machine translation and document summarization) by using different training datasets.\n",
    "\n",
    "<img src=\"https://github.com/nlp-236299/data/raw/master/img/encoderdecoder_attn_1layer.png\" alt=\"encoder-decoder-attn illustration\" />\n",
    "\n",
    "Implement the class `AttnEncoderDecoder` to convert natural language queries into SQL statements. You may find that you can reuse most of the code you wrote for lab 4-5. A reasonable way to proceed is to implement the following methods:\n",
    "\n",
    "* **Model**\n",
    "\n",
    "    1. `__init__`: an initializer where you create network modules.\n",
    "\n",
    "    2. `forward`: given source word ids of size `(batch_size, max_src_len)`, source lengths of size `(batch_size)` and decoder input target word ids `(batch_size, max_tgt_len)`, returns logits `(batch_size, max_tgt_len, V_tgt)`. For better modularity you might want to implement it by implementing two functions `forward_encoder` and `forward_decoder`.\n",
    "\n",
    "* **Optimization**\n",
    "\n",
    "    3. `train_all`: compute loss on training data, compute gradients, and update model parameters to minimize the loss.\n",
    "\n",
    "    4. `evaluate_ppl`: evaluate the current model's perplexity on a given dataset iterator, we use the perplexity value on the validation set to select the best model.\n",
    "\n",
    "* **Decoding**\n",
    "\n",
    "    5. `predict`: Generates the target sequence given a list of source tokens using beam search decoding. Note that here you can assume the batch size to be 1 for simplicity."
   ],
   "id": "edce39ff"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "f48b358e"
   },
   "outputs": [],
   "source": [
    "def attention(batched_Q, batched_K, batched_V, mask=None):\n",
    "  \"\"\"\n",
    "  Performs the attention operation and returns the attention matrix\n",
    "  `batched_A` and the context matrix `batched_C` using queries\n",
    "  `batched_Q`, keys `batched_K`, and values `batched_V`.\n",
    "\n",
    "  Arguments:\n",
    "      batched_Q: (bsz, q_len, D)\n",
    "      batched_K: (bsz, k_len, D)\n",
    "      batched_V: (bsz, k_len, D)\n",
    "      mask: (bsz, q_len, k_len). An optional boolean mask *disallowing*\n",
    "            attentions where the mask value is *`False`*.\n",
    "  Returns:\n",
    "      batched_A: the normalized attention scores (bsz, q_len, k_len)\n",
    "      batched_C: a tensor of size (bsz, q_len, D).\n",
    "  \"\"\"\n",
    "  # Check sizes\n",
    "  D = batched_Q.size(-1)\n",
    "  bsz = batched_Q.size(0)\n",
    "  q_len = batched_Q.size(1)\n",
    "  k_len = batched_K.size(1)\n",
    "  assert batched_K.size(-1) == D and batched_V.size(-1) == D\n",
    "  assert batched_K.size(0) == bsz and batched_V.size(0) == bsz\n",
    "  assert batched_V.size(1) == k_len\n",
    "  if mask is not None:\n",
    "    assert mask.size() == torch.Size([bsz, q_len, k_len])\n",
    "  prevA = torch.bmm(batched_Q, torch.transpose(batched_K, 1, 2))\n",
    "  if mask is not None:\n",
    "      prevA[mask == 0] = -float('inf')\n",
    "\n",
    "  batched_A = torch.softmax(prevA, -1)\n",
    "  batched_C = torch.bmm(batched_A, batched_V)\n",
    "  # Verify that things sum up to one properly.\n",
    "  assert torch.all(torch.isclose(batched_A.sum(-1),\n",
    "                                 torch.ones(bsz, q_len).to(device)))\n",
    "  return batched_A, batched_C"
   ],
   "id": "f48b358e"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "04de42e3"
   },
   "outputs": [],
   "source": [
    "class Beam():\n",
    "  \"\"\"\n",
    "  Helper class for storing a hypothesis, its score and its decoder hidden state.\n",
    "  \"\"\"\n",
    "  def __init__(self, decoder_state, tokens, score):\n",
    "    self.decoder_state = decoder_state\n",
    "    self.tokens = tokens\n",
    "    self.score = score\n",
    "\n",
    "class BeamSearcher():\n",
    "  \"\"\"\n",
    "  Main class for beam search.\n",
    "  \"\"\"\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    self.bos_id = model.bos_id\n",
    "    self.eos_id = model.eos_id\n",
    "    self.padding_id_src = model.padding_id_src\n",
    "    self.V = model.V_tgt\n",
    "\n",
    "\n",
    "  def beam_search(self, src, src_lengths, K, max_T):\n",
    "    \"\"\"\n",
    "    Performs beam search decoding.\n",
    "    Arguments:\n",
    "        src: src batch of size (1, max_src_len)\n",
    "        src_lengths: src lengths of size (1)\n",
    "        K: beam size\n",
    "        max_T: max possible target length considered\n",
    "    Returns:\n",
    "        a list of token ids and a list of attentions\n",
    "    \"\"\"\n",
    "    finished = []\n",
    "    all_attns = []\n",
    "    # Initialize the beam\n",
    "    self.model.eval()\n",
    "    #TODO - fill in `memory_bank`, `encoder_final_state`, and `init_beam` below\n",
    "    # memory_bank = ...\n",
    "    # encoder_final_state = ...\n",
    "    # init_beam = ...\n",
    "    memory_bank , encoder_final_state = self.model.forward_encoder(src, src_lengths)\n",
    "    init_beam = Beam(encoder_final_state, torch.tensor([self.bos_id], dtype=torch.int64).to(device), 0)\n",
    "    beams = [init_beam]\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for t in range(max_T): # main body of search over time steps\n",
    "\n",
    "        # Expand each beam by all possible tokens y_{t+1}\n",
    "        all_total_scores = []\n",
    "        for beam in beams:\n",
    "          y_1_to_t, score, decoder_state = beam.tokens, beam.score, beam.decoder_state\n",
    "          y_t = y_1_to_t[-1]\n",
    "          #TODO - finish the code below\n",
    "          # Hint: you might want to use `model.forward_decoder_incrementally` with `normalize=True`\n",
    "          src_mask = src.ne(self.padding_id_src)\n",
    "          # ...\n",
    "          # logits = ...\n",
    "          # decoder_state = ...\n",
    "          # attn = ...\n",
    "          # total_scores = ...\n",
    "          logits, decoder_state, attn = self.model.forward_decoder_incrementally(decoder_state, y_t.unsqueeze(dim=0), memory_bank, src_mask, True)\n",
    "                                                                                #  torch.as_tensor([y_t], device=device), memory_bank, src_mask)\n",
    "          total_scores = score + logits\n",
    "          all_total_scores.append(total_scores)\n",
    "          all_attns.append(attn) # keep attentions for visualization\n",
    "          beam.decoder_state = decoder_state # update decoder state in the beam\n",
    "        all_total_scores = torch.stack(all_total_scores) # (K, V) when t>0, (1, V) when t=0\n",
    "\n",
    "        # Find K best next beams\n",
    "        # The code below has the same functionality as line 6-12, but is more efficient\n",
    "        all_scores_flattened = all_total_scores.view(-1) # K*V when t>0, 1*V when t=0\n",
    "        topk_scores, topk_ids = all_scores_flattened.topk(K, 0)\n",
    "        beam_ids = topk_ids.div(self.V, rounding_mode='floor')\n",
    "        next_tokens = topk_ids - beam_ids * self.V\n",
    "        new_beams = []\n",
    "        for k in range(K):\n",
    "          beam_id = beam_ids[k]       # which beam it comes from\n",
    "          y_t_plus_1 = next_tokens[k] # which y_{t+1}\n",
    "          score = topk_scores[k]\n",
    "          beam = beams[beam_id]\n",
    "          decoder_state = beam.decoder_state\n",
    "          y_1_to_t = beam.tokens\n",
    "          #TODO\n",
    "          # new_beam = ...\n",
    "          new_beam = Beam(decoder_state, torch.cat((y_1_to_t, y_t_plus_1.unsqueeze(0)), dim=0), score)\n",
    "          new_beams.append(new_beam)\n",
    "        beams = new_beams\n",
    "\n",
    "        # Set aside completed beams\n",
    "        # TODO - move completed beams to `finished` (and remove them from `beams`)\n",
    "        # Set aside completed beams\n",
    "        for beam in beams:\n",
    "            if beam.tokens[-1] == self.eos_id:\n",
    "                finished.append(beam)\n",
    "                beams.remove(beam)\n",
    "\n",
    "        # Break the loop if everything is completed\n",
    "        if len(beams) == 0:\n",
    "            break\n",
    "\n",
    "    # Return the best hypothesis\n",
    "    if len(finished) > 0:\n",
    "      finished = sorted(finished, key=lambda beam: -beam.score)\n",
    "      return [token.item() for token in finished[0].tokens], all_attns\n",
    "    else: # when nothing is finished, return an unfinished hypothesis\n",
    "      return [token.item() for token in beams[0].tokens], all_attns"
   ],
   "id": "04de42e3"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "imTkmixRBc6L"
   },
   "outputs": [],
   "source": [
    "#TODO - implement the `AttnEncoderDecoder` class.\n",
    "class AttnEncoderDecoder(nn.Module):\n",
    "  def __init__(self, hf_src_tokenizer, hf_tgt_tokenizer, hidden_size=64, layers=3):\n",
    "    \"\"\"\n",
    "    Initializer. Creates network modules and loss function.\n",
    "    Arguments:\n",
    "        hf_src_tokenizer: hf src tokenizer\n",
    "        hf_tgt_tokenizer: hf tgt tokenizer\n",
    "        hidden_size: hidden layer size of both encoder and decoder\n",
    "        layers: number of layers of both encoder and decoder\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.hf_src_tokenizer = hf_src_tokenizer\n",
    "    self.hf_tgt_tokenizer = hf_tgt_tokenizer\n",
    "\n",
    "    # Keep the vocabulary sizes available\n",
    "    self.V_src = len(self.hf_src_tokenizer)\n",
    "    self.V_tgt = len(self.hf_tgt_tokenizer)\n",
    "\n",
    "    # Get special word ids\n",
    "    self.padding_id_src = self.hf_src_tokenizer.pad_token_id\n",
    "    self.padding_id_tgt = self.hf_tgt_tokenizer.pad_token_id\n",
    "    self.bos_id = self.hf_tgt_tokenizer.bos_token_id\n",
    "    self.eos_id = self.hf_tgt_tokenizer.eos_token_id\n",
    "\n",
    "    # Keep hyper-parameters available\n",
    "    self.embedding_size = hidden_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.layers = layers\n",
    "\n",
    "    # Create essential modules\n",
    "    self.word_embeddings_src = nn.Embedding(self.V_src, self.embedding_size)\n",
    "    self.word_embeddings_tgt = nn.Embedding(self.V_tgt, self.embedding_size)\n",
    "\n",
    "    # RNN cells\n",
    "    self.encoder_rnn = nn.LSTM(\n",
    "      input_size    = self.embedding_size,\n",
    "      hidden_size   = hidden_size // 2, # to match decoder hidden size\n",
    "      num_layers    = layers,\n",
    "      batch_first=True,\n",
    "      bidirectional = True              # bidirectional encoder\n",
    "    )\n",
    "    self.decoder_rnn = nn.LSTM(\n",
    "      input_size    = self.embedding_size,\n",
    "      hidden_size   = hidden_size,\n",
    "      num_layers    = layers,\n",
    "      batch_first=True,\n",
    "      bidirectional = False             # unidirectional decoder\n",
    "    )\n",
    "\n",
    "    # Final projection layer\n",
    "    self.hidden2output = nn.Linear(2*hidden_size, self.V_tgt) # project the concatenation to logits\n",
    "\n",
    "    # Create loss function\n",
    "    self.loss_function = nn.CrossEntropyLoss(reduction='sum',\n",
    "                                             ignore_index=self.padding_id_tgt)\n",
    "\n",
    "  def forward_encoder(self, src, src_lengths):\n",
    "    \"\"\"\n",
    "    Encodes source words `src`.\n",
    "    Arguments:\n",
    "        src: src batch of size (bsz, max_src_len)\n",
    "        src_lengths: src lengths of size (bsz)\n",
    "    Returns:\n",
    "        memory_bank: a tensor of size (bsz, src_len, hidden_size)\n",
    "        (final_state, context): `final_state` is a tuple (h, c) where h/c is of size\n",
    "                                (layers, bsz, hidden_size), and `context` is `None`.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    # Convert src_lengths to CPU to avoid compatibility issues\n",
    "    src_lengths = src_lengths.cpu()\n",
    "\n",
    "    # Pack the source sequence using src_lengths\n",
    "    packed_src = pack(self.word_embeddings_src(src), src_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    # Apply the RNN to the packed source sequence\n",
    "    packed_memory_bank, (h, c) = self.encoder_rnn(packed_src)\n",
    "\n",
    "    # Unpack the packed memory bank\n",
    "    memory_bank, _ = unpack(packed_memory_bank)\n",
    "    # memory_bank, _ = unpack(packed_memory_bank, batch_first=True)\n",
    "\n",
    "\n",
    "    def reshape(x):\n",
    "        size1 = (self.layers, 2, src.shape[0], self.hidden_size // 2)\n",
    "        size2 = (self.layers, src.shape[0], self.hidden_size)\n",
    "        return x.reshape(*size1).transpose(1, 2).reshape(*size2)\n",
    "    final_state = (reshape(h), reshape(c))\n",
    "    context = None\n",
    "    ## Added this:\n",
    "    memory_bank = memory_bank.permute((1, 0, 2))\n",
    "\n",
    "    return memory_bank, (final_state, context)\n",
    "\n",
    "  def forward_decoder(self, encoder_final_state, tgt_in, memory_bank, src_mask):\n",
    "    \"\"\"\n",
    "    Decodes based on encoder final state, memory bank, src_mask, and ground truth\n",
    "    target words.\n",
    "    Arguments:\n",
    "        encoder_final_state: (final_state, None) where final_state is the encoder\n",
    "                             final state used to initialize decoder. None is the\n",
    "                             initial context (there's no previous context at the\n",
    "                             first step).\n",
    "        tgt_in: a tensor of size (bsz, tgt_len)\n",
    "        memory_bank: a tensor of size (bsz, src_len, hidden_size), encoder outputs\n",
    "                     at every position\n",
    "        src_mask: a tensor of size (bsz, src_len): a boolean tensor, `False` where\n",
    "                  src is padding (we disallow decoder to attend to those places).\n",
    "    Returns:\n",
    "        Logits of size (bsz, tgt_len, V_tgt) (before the softmax operation)\n",
    "    \"\"\"\n",
    "    max_tgt_length = tgt_in.size(1)\n",
    "\n",
    "    # Initialize decoder state, note that it's a tuple (state, context) here\n",
    "    decoder_states = encoder_final_state\n",
    "\n",
    "    all_logits = []\n",
    "    for i in range(max_tgt_length):\n",
    "      logits, decoder_states, attn = \\\n",
    "        self.forward_decoder_incrementally(decoder_states,\n",
    "                                           tgt_in[:, i],\n",
    "                                           memory_bank,\n",
    "                                           src_mask,\n",
    "                                           normalize=False)\n",
    "      all_logits.append(logits)             # list of bsz, vocab_tgt\n",
    "    all_logits = torch.stack(all_logits, 1) # bsz, tgt_len, vocab_tgt\n",
    "    return all_logits\n",
    "\n",
    "  def forward(self, src, src_lengths, tgt_in):\n",
    "    \"\"\"\n",
    "    Performs forward computation, returns logits.\n",
    "    Arguments:\n",
    "        src: src batch of size (bsz, max_src_len)\n",
    "        src_lengths: src lengths of size (bsz)\n",
    "        tgt_in:  a tensor of size (bsz, tgt_len)\n",
    "    \"\"\"\n",
    "    src_mask = src.ne(self.padding_id_src) # bsz, max_src_len\n",
    "    # Forward encoder\n",
    "    memory_bank, encoder_final_state = self.forward_encoder(src, src_lengths)\n",
    "    # Forward decoder\n",
    "    logits = self.forward_decoder(encoder_final_state, tgt_in, memory_bank, src_mask)\n",
    "    return logits\n",
    "\n",
    "  def forward_decoder_incrementally(self, prev_decoder_states, tgt_in_onestep,\n",
    "                                    memory_bank, src_mask,\n",
    "                                    normalize=True):\n",
    "    \"\"\"\n",
    "    Forward the decoder for a single step with token `tgt_in_onestep`.\n",
    "    This function will be used both in `forward_decoder` and in beam search.\n",
    "    Note that bsz can be greater than 1.\n",
    "    Arguments:\n",
    "        prev_decoder_states: a tuple (prev_decoder_state, prev_context). `prev_context`\n",
    "                             is `None` for the first step\n",
    "        tgt_in_onestep: a tensor of size (bsz), tokens at one step\n",
    "        memory_bank: a tensor of size (bsz, src_len, hidden_size), encoder outputs\n",
    "                     at every position\n",
    "        src_mask: a tensor of size (bsz, src_len): a boolean tensor, `False` where\n",
    "                  src is padding (we disallow decoder to attend to those places).\n",
    "        normalize: use log_softmax to normalize or not. Beam search needs to normalize,\n",
    "                   while `forward_decoder` does not\n",
    "    Returns:\n",
    "        logits: log probabilities for `tgt_in_token` of size (bsz, V_tgt)\n",
    "        decoder_states: (`decoder_state`, `context`) which will be used for the\n",
    "                        next incremental update\n",
    "        attn: normalized attention scores at this step (bsz, src_len)\n",
    "    \"\"\"\n",
    "    prev_decoder_state, prev_context = prev_decoder_states\n",
    "    #TODO\n",
    "    # word_embeddings = self.word_embeddings_tgt(tgt_in_onestep).unsqueeze(1)\n",
    "    word_embeddings = self.word_embeddings_tgt(tgt_in_onestep)\n",
    "\n",
    "    if prev_context is not None:\n",
    "        word_embeddings += prev_context\n",
    "\n",
    "\n",
    "    prev_decoder_state = (prev_decoder_state[0].contiguous(), prev_decoder_state[1].contiguous())\n",
    "    packed_batchMat, decoder_state = self.decoder_rnn(word_embeddings.unsqueeze(dim=1), prev_decoder_state)\n",
    "\n",
    "\n",
    "    attn, context = attention(packed_batchMat, memory_bank, memory_bank, src_mask.unsqueeze(1))\n",
    "    # logits = self.hidden2output(torch.cat((context, packed_batchMat), dim=-1))\n",
    "\n",
    "    attn = attn.squeeze(1)\n",
    "    context = context.squeeze(1)\n",
    "    packed_batchMat = packed_batchMat.squeeze(1)\n",
    "\n",
    "    decoder_states = (decoder_state, context)\n",
    "\n",
    "    ## Added:\n",
    "    logits = self.hidden2output(torch.cat([packed_batchMat, context], dim=-1))\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "      logits = torch.log_softmax(logits, dim=-1)\n",
    "    return logits, decoder_states, attn\n",
    "\n",
    "  def evaluate_ppl(self, iterator):\n",
    "    \"\"\"Returns the model's perplexity on a given dataset `iterator`.\"\"\"\n",
    "    # Switch to eval mode\n",
    "    self.eval()\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "    for batch in iterator:\n",
    "      # Input and target\n",
    "      src = batch['src_ids']              # bsz, max_src_len\n",
    "      src_lengths = batch['src_lengths']  # bsz\n",
    "      tgt_in = batch['tgt_ids'][:, :-1] # Remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
    "      tgt_out = batch['tgt_ids'][:, 1:] # Remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
    "      # Forward to get logits\n",
    "      logits = self.forward(src, src_lengths, tgt_in) # bsz, tgt_len, V_tgt\n",
    "      # Compute cross entropy loss\n",
    "      loss = self.loss_function(logits.reshape(-1, self.V_tgt), tgt_out.reshape(-1))\n",
    "      total_loss += loss.item()\n",
    "      total_words += tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
    "    return math.exp(total_loss/total_words)\n",
    "\n",
    "  def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Switch the module to training mode\n",
    "    self.train()\n",
    "    # Use Adam to optimize the parameters\n",
    "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    best_validation_ppl = float('inf')\n",
    "    best_model = None\n",
    "    # Run the optimization for multiple epochs\n",
    "    for epoch in range(epochs):\n",
    "      total_words = 0\n",
    "      total_loss = 0.0\n",
    "\n",
    "      for batch in tqdm(train_iter):\n",
    "        # Zero the parameter gradients\n",
    "        self.zero_grad()\n",
    "        # Input and target\n",
    "        tgt = batch['tgt_ids']              # bsz, max_tgt_len\n",
    "        src = batch['src_ids']              # bsz, max_src_len\n",
    "        src_lengths = batch['src_lengths']  # bsz\n",
    "        tgt_in = tgt[:, :-1].contiguous() # Remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
    "        tgt_out = tgt[:, 1:].contiguous() # Remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
    "        bsz = tgt.size(0)\n",
    "        # Run forward pass and compute loss along the way.\n",
    "        logits = self.forward(src, src_lengths, tgt_in)\n",
    "        loss = self.loss_function(logits.view(-1, self.V_tgt), tgt_out.view(-1))\n",
    "\n",
    "        # Training stats\n",
    "        num_tgt_words = tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
    "        total_words += num_tgt_words\n",
    "        total_loss += loss.item()\n",
    "        # Perform backpropagation\n",
    "        loss.div(bsz).backward()\n",
    "        optim.step()\n",
    "\n",
    "      # Evaluate and track improvements on the validation dataset\n",
    "      validation_ppl = self.evaluate_ppl(val_iter)\n",
    "      self.train()\n",
    "      if validation_ppl < best_validation_ppl:\n",
    "        best_validation_ppl = validation_ppl\n",
    "        self.best_model = copy.deepcopy(self.state_dict())\n",
    "      epoch_loss = total_loss / total_words\n",
    "      print (f'Epoch: {epoch} Training Perplexity: {math.exp(epoch_loss):.4f} '\n",
    "             f'Validation Perplexity: {validation_ppl:.4f}')\n",
    "\n",
    "  def predict(self, tokens, K, max_T):\n",
    "    beam_searcher = BeamSearcher(self)\n",
    "    ## Adjust tokens to fit for BeamSearcher\n",
    "    tokens = self.hf_src_tokenizer.encode(tokens)\n",
    "    tokens_length = torch.LongTensor([len(tokens)]).to(device)\n",
    "    tokens = torch.LongTensor(tokens)\n",
    "    tokens = tokens.unsqueeze(0).to(device)\n",
    "    prediction, _ = beam_searcher.beam_search(tokens, tokens_length, K, max_T=max_T)\n",
    "    # Convert to string\n",
    "    prediction = self.hf_tgt_tokenizer.decode(prediction, skip_special_tokens=True)\n",
    "    return prediction"
   ],
   "id": "imTkmixRBc6L"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3ff6c4a"
   },
   "source": [
    "We provide the recommended hyperparameters for the final model in the script below, but you are free to tune the hyperparameters or change any part of the provided code.\n",
    "\n",
    ">For quick debugging, we recommend starting with smaller models (by using a very small `hidden_size`), and only a single epoch. If the model runs smoothly, then you can train the full model on GPUs."
   ],
   "id": "b3ff6c4a"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "579f35f5",
    "outputId": "15714767-81d5-4ffc-dcf9-036cd587c5d0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:41<00:00,  2.25it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0 Training Perplexity: 4.1195 Validation Perplexity: 1.6977\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 Training Perplexity: 1.4546 Validation Perplexity: 1.3618\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:37<00:00,  2.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 2 Training Perplexity: 1.2739 Validation Perplexity: 1.2620\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:39<00:00,  2.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 3 Training Perplexity: 1.1957 Validation Perplexity: 1.2016\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 4 Training Perplexity: 1.1509 Validation Perplexity: 1.1726\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:37<00:00,  2.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 5 Training Perplexity: 1.1203 Validation Perplexity: 1.1478\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:36<00:00,  2.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 6 Training Perplexity: 1.0984 Validation Perplexity: 1.1386\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 7 Training Perplexity: 1.0817 Validation Perplexity: 1.1221\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:35<00:00,  2.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 8 Training Perplexity: 1.0675 Validation Perplexity: 1.1166\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 9 Training Perplexity: 1.0572 Validation Perplexity: 1.1098\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:37<00:00,  2.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 10 Training Perplexity: 1.0513 Validation Perplexity: 1.1062\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:34<00:00,  2.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 11 Training Perplexity: 1.0428 Validation Perplexity: 1.1003\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:36<00:00,  2.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 12 Training Perplexity: 1.0373 Validation Perplexity: 1.0967\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:39<00:00,  2.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 13 Training Perplexity: 1.0320 Validation Perplexity: 1.1016\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:36<00:00,  2.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 14 Training Perplexity: 1.0294 Validation Perplexity: 1.0937\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:39<00:00,  2.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 15 Training Perplexity: 1.0265 Validation Perplexity: 1.1009\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:37<00:00,  2.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 16 Training Perplexity: 1.0224 Validation Perplexity: 1.0940\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:39<00:00,  2.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 17 Training Perplexity: 1.0180 Validation Perplexity: 1.0945\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:39<00:00,  2.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 18 Training Perplexity: 1.0162 Validation Perplexity: 1.0929\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 19 Training Perplexity: 1.0148 Validation Perplexity: 1.0964\n",
      "Validation perplexity: 1.093\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20 # epochs; we recommend starting with a smaller number like 1\n",
    "LEARNING_RATE = 1e-4 # learning rate\n",
    "checkpoint_filename = f'goal2_e_{EPOCHS}'\n",
    "# Instantiate and train classifier\n",
    "model = AttnEncoderDecoder(hf_src_tokenizer, hf_tgt_tokenizer,\n",
    "  hidden_size    = 1024,\n",
    "  layers         = 1,\n",
    ").to(device)\n",
    "\n",
    "folder_name = \"models\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "if os.path.isfile(f'models/{checkpoint_filename}.pt'):\n",
    "    print(f'*** Loading model from checkpoint file {checkpoint_filename}')\n",
    "    model.load_state_dict(torch.load(f'models/{checkpoint_filename}.pt'))\n",
    "else:\n",
    "    model.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "    model.load_state_dict(model.best_model)\n",
    "    torch.save(model.state_dict(), f'models/{checkpoint_filename}.pt')\n",
    "\n",
    "# model.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "# model.load_state_dict(model.best_model)\n",
    "\n",
    "# Evaluate model performance, the expected value should be < 1.2\n",
    "print (f'Validation perplexity: {model.evaluate_ppl(val_iter):.3f}')"
   ],
   "id": "579f35f5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03ee437f"
   },
   "source": [
    "With a trained model, we can convert questions to SQL statements. We recommend making sure that the model can generate at least reasonable results on the examples from before, before evaluating on the full test set."
   ],
   "id": "03ee437f"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "dccc5761"
   },
   "outputs": [],
   "source": [
    "def seq2seq_trial(sentence, gold_sql):\n",
    "  print(\"Sentence: \", sentence, \"\\n\")\n",
    "\n",
    "  predicted_sql = model.predict(sentence, K=1, max_T=400)\n",
    "  print(\"Predicted SQL:\\n\\n\", predicted_sql, \"\\n\")\n",
    "\n",
    "  if verify(predicted_sql, gold_sql, silent=False):\n",
    "    print ('Correct!')\n",
    "  else:\n",
    "    print ('Incorrect!')"
   ],
   "id": "dccc5761"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7114b61b",
    "outputId": "907888d1-6cbf-42aa-dd12-514f733162ed"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  flights from phoenix to milwaukee \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight_1.flight_id FROM flight flight_1, airport_service airport_service_1, city city_1, airport_service airport_service_2, city city_2 WHERE flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'PHOENIX' AND flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'MILWAUKEE' \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(108086,), (108087,), (301763,), (301764,), (301765,), (301766,), (302323,), (304881,), (310619,), (310620,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(108086,), (108087,), (301763,), (301764,), (301765,), (301766,), (302323,), (304881,), (310619,), (310620,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_trial(example_1, gold_sql_1)"
   ],
   "id": "7114b61b"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c13fcf3",
    "outputId": "89306e73-d0ac-4b73-d8ed-89c9352baf8e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  i would like a united flight \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight_1.flight_id FROM flight flight_1, airport_service airport_service_1, city city_1 WHERE flight_1.airline_code = 'UA' AND flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'DENVER' \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(100094,), (100099,), (100699,), (100703,), (100704,), (100705,), (100706,), (101082,), (101083,), (101084,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(100094,), (100099,), (100145,), (100158,), (100164,), (100167,), (100169,), (100203,), (100204,), (100296,)] \n",
      "\n",
      "Incorrect!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_trial(example_2, gold_sql_2)"
   ],
   "id": "8c13fcf3"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cabdfeae",
    "outputId": "01f3aede-d5f3-4dad-a264-25c6d95f0152"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  i would like a flight between boston and dallas \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight_1.flight_id FROM flight flight_1, airport_service airport_service_1, city city_1, airport_service airport_service_2, city city_2 WHERE flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'BOSTON' AND flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'DALLAS' \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(103171,), (103172,), (103173,), (103174,), (103175,), (103176,), (103177,), (103178,), (103179,), (103180,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(103171,), (103172,), (103173,), (103174,), (103175,), (103176,), (103177,), (103178,), (103179,), (103180,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_trial(example_3, gold_sql_3)"
   ],
   "id": "cabdfeae"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "560b218c",
    "outputId": "f4153b12-427d-408b-df18-6eb2b168a192"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  show me the united flights from denver to baltimore \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight_1.flight_id FROM flight flight_1, airport_service airport_service_1, city city_1, airport_service airport_service_2, city city_2 WHERE flight_1.airline_code = 'UA' AND ( flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'DENVER' AND flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'BALTIMORE' ) \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(101231,), (101233,), (305983,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(101231,), (101233,), (305983,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_trial(example_4, gold_sql_4)"
   ],
   "id": "560b218c"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "069d8ba3",
    "outputId": "3ac169c7-301e-4eda-ec5f-6b37f1c5087a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  show flights from cleveland to miami that arrive before 4pm \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight_1.flight_id FROM flight flight_1, airport_service airport_service_1, city city_1, airport_service airport_service_2, city city_2 WHERE flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'CLEVELAND' AND ( flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'MIAMI' AND flight_1.arrival_time < 1600 ) \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(107698,), (301117,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(107698,), (301117,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_trial(example_5, gold_sql_5)"
   ],
   "id": "069d8ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d042abfe",
    "outputId": "02fa4208-f409-47f8-f6c9-3ac263c7376a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  okay how about a flight on sunday from tampa to charlotte \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight_1.flight_id FROM flight flight_1, airport_service airport_service_1, city city_1, airport_service airport_service_2, city city_2, days days_1, date_day date_day_1 WHERE flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'TAMPA' AND ( flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'CHARLOTTE' AND flight_1.flight_days = days_1.days_code AND days_1.day_name = date_day_1.day_name AND date_day_1.year = 1991 AND date_day_1.month_number = 8 AND date_day_1.day_number = 27 ) \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(101860,), (101861,), (101862,), (101863,), (101864,), (101865,), (305231,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(101860,), (101861,), (101862,), (101863,), (101864,), (101865,), (305231,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_trial(example_6, gold_sql_6b)"
   ],
   "id": "d042abfe"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ef3fed0",
    "outputId": "2c3ec891-9198-46fc-8f6b-a04f685c1411"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  list all flights going from boston to atlanta that leaves before 7 am on thursday \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight_1.flight_id FROM flight flight_1, airport_service airport_service_1, city city_1, airport_service airport_service_2, city city_2, days days_1, date_day date_day_1 WHERE flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'BOSTON' AND ( flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'ATLANTA' AND ( flight_1.flight_days = days_1.days_code AND days_1.day_name = date_day_1.day_name AND date_day_1.year = 1991 AND date_day_1.month_number = 5 AND date_day_1.day_number = 24 AND flight_1.departure_time < 700 ) ) \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(100014,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(100014,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_trial(example_7, gold_sql_7b)"
   ],
   "id": "3ef3fed0"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f3719f9",
    "outputId": "5f08f6b6-9806-4f28-8e65-2779a89a855d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:  list the flights from dallas to san francisco on american airlines \n",
      "\n",
      "Predicted SQL:\n",
      "\n",
      " SELECT DISTINCT flight_1.flight_id FROM flight flight_1, airport_service airport_service_1, city city_1, airport_service airport_service_2, city city_2 WHERE flight_1.airline_code = 'AA' AND ( flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'DALLAS' AND flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'SAN FRANCISCO' ) \n",
      "\n",
      "Predicted DB result:\n",
      "\n",
      " [(108452,), (108454,), (108456,), (111083,), (111085,), (111086,), (111090,), (111091,), (111092,), (111094,)] \n",
      "\n",
      "Gold DB result:\n",
      "\n",
      " [(108452,), (108454,), (108456,), (111083,), (111085,), (111086,), (111090,), (111091,), (111092,), (111094,)] \n",
      "\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_trial(example_8, gold_sql_8)"
   ],
   "id": "5f3719f9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a5ee90c"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Now we are ready to run the full evaluation. A proper implementation should reach more than 35% precision/recall/F1."
   ],
   "id": "6a5ee90c"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "2fb44bbf"
   },
   "outputs": [],
   "source": [
    "def seq2seq_predictor(tokens):\n",
    "  prediction = model.predict(tokens, K=1, max_T=400)\n",
    "  return prediction"
   ],
   "id": "2fb44bbf"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e8a2323",
    "outputId": "46fc3d62-05b7-42c6-e8db-87efd75da431"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "332it [01:22,  4.00it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "precision: 0.39\n",
      "recall:    0.39\n",
      "F1:        0.39\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = evaluate(seq2seq_predictor, test_data, num_examples=0)\n",
    "print(f\"precision: {precision:3.2f}\")\n",
    "print(f\"recall:    {recall:3.2f}\")\n",
    "print(f\"F1:        {f1:3.2f}\")"
   ],
   "id": "5e8a2323"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f59bab43"
   },
   "source": [
    "## Goal 3: Implement a seq2seq model (with cross attention and self attention)\n",
    "\n",
    "In the previous section, you have implemented a seq2seq model with attention. The attention mechanism used in that section is usually referred to as \"cross-attention\", as at each decoding step, the decoder attends to encoder outputs, enabling a dynamic view on the encoder side as decoding proceeds.\n",
    "\n",
    "Similarly, we can have a dynamic view on the decoder side as well as decoding proceeds, i.e., the decoder attends to decoder outputs at previous steps. This is called \"self attention\", and has been found very useful in modern neural architectures such as transformers.\n",
    "\n",
    "Augment the seq2seq model you implemented before with a decoder self-attention mechanism as class `AttnEncoderDecoder2`. A model diagram can be found below:\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/nlp-236299/data/raw/master/img/encoderdecoder_attn_selfattn.png\" alt=\"encoder-decoder-attn-with-selfattn illustration\" />\n",
    "\n",
    "At each decoding step, the decoder LSTM first produces an output state $o_t$, then it attends to all previous output states $o_1, \\ldots, o_{t-1}$ (decoder self-attention). You need to special case the first decoding step to not perform self-attention, as there are no previous decoder states. The attention result is added to $o_t$ itself and the sum is used as $q_t$ to attend to the encoder side (encoder-decoder cross-attention). The rest of the model is the same as encoder-decoder with attention.\n"
   ],
   "id": "f59bab43"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "65ec5d9d"
   },
   "outputs": [],
   "source": [
    "#TODO - implement the `AttnEncoderDecoder2` class.\n",
    "class AttnEncoderDecoder2(nn.Module):\n",
    "  def __init__(self, hf_src_tokenizer, hf_tgt_tokenizer, hidden_size=64, layers=3):\n",
    "    \"\"\"\n",
    "    Initializer. Creates network modules and loss function.\n",
    "    Arguments:\n",
    "        hf_src_tokenizer: hf src tokenizer\n",
    "        hf_tgt_tokenizer: hf tgt tokenizer\n",
    "        hidden_size: hidden layer size of both encoder and decoder\n",
    "        layers: number of layers of both encoder and decoder\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.hf_src_tokenizer = hf_src_tokenizer\n",
    "    self.hf_tgt_tokenizer = hf_tgt_tokenizer\n",
    "\n",
    "    # Keep the vocabulary sizes available\n",
    "    self.V_src = len(self.hf_src_tokenizer)\n",
    "    self.V_tgt = len(self.hf_tgt_tokenizer)\n",
    "\n",
    "    # Get special word ids\n",
    "    self.padding_id_src = self.hf_src_tokenizer.pad_token_id\n",
    "    self.padding_id_tgt = self.hf_tgt_tokenizer.pad_token_id\n",
    "    self.bos_id = self.hf_tgt_tokenizer.bos_token_id\n",
    "    self.eos_id = self.hf_tgt_tokenizer.eos_token_id\n",
    "\n",
    "    # Keep hyper-parameters available\n",
    "    self.embedding_size = hidden_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.layers = layers\n",
    "\n",
    "    # Create essential modules\n",
    "    self.word_embeddings_src = nn.Embedding(self.V_src, self.embedding_size)\n",
    "    self.word_embeddings_tgt = nn.Embedding(self.V_tgt, self.embedding_size)\n",
    "\n",
    "    # RNN cells\n",
    "    self.encoder_rnn = nn.LSTM(\n",
    "      input_size    = self.embedding_size,\n",
    "      hidden_size   = hidden_size // 2, # to match decoder hidden size\n",
    "      num_layers    = layers,\n",
    "      batch_first=True,\n",
    "      bidirectional = True              # bidirectional encoder\n",
    "    )\n",
    "    self.decoder_rnn = nn.LSTM(\n",
    "      input_size    = self.embedding_size,\n",
    "      hidden_size   = hidden_size,\n",
    "      num_layers    = layers,\n",
    "      batch_first=True,\n",
    "      bidirectional = False             # unidirectional decoder\n",
    "    )\n",
    "\n",
    "    # Final projection layer\n",
    "    self.hidden2output = nn.Linear(2*hidden_size, self.V_tgt) # project the concatenation to logits\n",
    "\n",
    "    # Create loss function\n",
    "    self.loss_function = nn.CrossEntropyLoss(reduction='sum',\n",
    "                                             ignore_index=self.padding_id_tgt)\n",
    "\n",
    "  def forward_encoder(self, src, src_lengths):\n",
    "    \"\"\"\n",
    "    Encodes source words `src`.\n",
    "    Arguments:\n",
    "        src: src batch of size (bsz, max_src_len)\n",
    "        src_lengths: src lengths of size (bsz)\n",
    "    Returns:\n",
    "        memory_bank: a tensor of size (bsz, src_len, hidden_size)\n",
    "        (final_state, context): `final_state` is a tuple (h, c) where h/c is of size\n",
    "                                (layers, bsz, hidden_size), and `context` is `None`.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    # Convert src_lengths to CPU to avoid compatibility issues\n",
    "    src_lengths = src_lengths.cpu()\n",
    "\n",
    "    # Pack the source sequence using src_lengths\n",
    "    packed_src = pack(self.word_embeddings_src(src), src_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    # Apply the RNN to the packed source sequence\n",
    "    packed_memory_bank, (h, c) = self.encoder_rnn(packed_src)\n",
    "\n",
    "    # Unpack the packed memory bank\n",
    "    memory_bank, _ = unpack(packed_memory_bank)\n",
    "    # memory_bank, _ = unpack(packed_memory_bank, batch_first=True)\n",
    "\n",
    "\n",
    "    def reshape(x):\n",
    "        size1 = (self.layers, 2, src.shape[0], self.hidden_size // 2)\n",
    "        size2 = (self.layers, src.shape[0], self.hidden_size)\n",
    "        return x.reshape(*size1).transpose(1, 2).reshape(*size2)\n",
    "    final_state = (reshape(h), reshape(c))\n",
    "    context = None\n",
    "    ## Added this:\n",
    "    memory_bank = memory_bank.permute((1, 0, 2))\n",
    "\n",
    "    return memory_bank, (final_state, context)\n",
    "\n",
    "  def forward_decoder(self, encoder_final_state, tgt_in, memory_bank, src_mask):\n",
    "    \"\"\"\n",
    "    Decodes based on encoder final state, memory bank, src_mask, and ground truth\n",
    "    target words.\n",
    "    Arguments:\n",
    "        encoder_final_state: (final_state, None) where final_state is the encoder\n",
    "                             final state used to initialize decoder. None is the\n",
    "                             initial context (there's no previous context at the\n",
    "                             first step).\n",
    "        tgt_in: a tensor of size (bsz, tgt_len)\n",
    "        memory_bank: a tensor of size (bsz, src_len, hidden_size), encoder outputs\n",
    "                     at every position\n",
    "        src_mask: a tensor of size (bsz, src_len): a boolean tensor, `False` where\n",
    "                  src is padding (we disallow decoder to attend to those places).\n",
    "    Returns:\n",
    "        Logits of size (bsz, tgt_len, V_tgt) (before the softmax operation)\n",
    "    \"\"\"\n",
    "    max_tgt_length = tgt_in.size(1)\n",
    "\n",
    "    # Initialize decoder state, note that it's a tuple (state, context) here\n",
    "    decoder_states = encoder_final_state\n",
    "\n",
    "    all_logits = []\n",
    "    self_bank = None\n",
    "    for i in range(max_tgt_length):\n",
    "      logits, decoder_states, attn, self_bank = \\\n",
    "        self.forward_decoder_incrementally(decoder_states,\n",
    "                                           tgt_in[:, i],\n",
    "                                           memory_bank,\n",
    "                                           src_mask,\n",
    "                                           self_bank,\n",
    "                                           normalize=False)\n",
    "      all_logits.append(logits)             # list of bsz, vocab_tgt\n",
    "    all_logits = torch.stack(all_logits, 1) # bsz, tgt_len, vocab_tgt\n",
    "    return all_logits\n",
    "\n",
    "  def forward(self, src, src_lengths, tgt_in):\n",
    "    \"\"\"\n",
    "    Performs forward computation, returns logits.\n",
    "    Arguments:\n",
    "        src: src batch of size (bsz, max_src_len)\n",
    "        src_lengths: src lengths of size (bsz)\n",
    "        tgt_in:  a tensor of size (bsz, tgt_len)\n",
    "    \"\"\"\n",
    "    src_mask = src.ne(self.padding_id_src) # bsz, max_src_len\n",
    "    # Forward encoder\n",
    "    memory_bank, encoder_final_state = self.forward_encoder(src, src_lengths)\n",
    "    # Forward decoder\n",
    "    logits = self.forward_decoder(encoder_final_state, tgt_in, memory_bank, src_mask)\n",
    "    return logits\n",
    "\n",
    "  def forward_decoder_incrementally(self, prev_decoder_states, tgt_in_onestep,\n",
    "                                    memory_bank, src_mask, self_bank,\n",
    "                                    normalize=True):\n",
    "    \"\"\"\n",
    "    Forward the decoder for a single step with token `tgt_in_onestep`.\n",
    "    This function will be used both in `forward_decoder` and in beam search.\n",
    "    Note that bsz can be greater than 1.\n",
    "    Arguments:\n",
    "        prev_decoder_states: a tuple (prev_decoder_state, prev_context). `prev_context`\n",
    "                             is `None` for the first step\n",
    "        tgt_in_onestep: a tensor of size (bsz), tokens at one step\n",
    "        memory_bank: a tensor of size (bsz, src_len, hidden_size), encoder outputs\n",
    "                     at every position\n",
    "        src_mask: a tensor of size (bsz, src_len): a boolean tensor, `False` where\n",
    "                  src is padding (we disallow decoder to attend to those places).\n",
    "        normalize: use log_softmax to normalize or not. Beam search needs to normalize,\n",
    "                   while `forward_decoder` does not\n",
    "    Returns:\n",
    "        logits: log probabilities for `tgt_in_token` of size (bsz, V_tgt)\n",
    "        decoder_states: (`decoder_state`, `context`) which will be used for the\n",
    "                        next incremental update\n",
    "        attn: normalized attention scores at this step (bsz, src_len)\n",
    "    \"\"\"\n",
    "    prev_decoder_state, prev_context = prev_decoder_states\n",
    "    #TODO\n",
    "    # word_embeddings = self.word_embeddings_tgt(tgt_in_onestep).unsqueeze(1)\n",
    "    word_embeddings = self.word_embeddings_tgt(tgt_in_onestep)\n",
    "\n",
    "    if prev_context is not None:\n",
    "        word_embeddings += prev_context\n",
    "\n",
    "\n",
    "    prev_decoder_state = (prev_decoder_state[0].contiguous(), prev_decoder_state[1].contiguous())\n",
    "    packed_batchMat, decoder_state = self.decoder_rnn(word_embeddings.unsqueeze(dim=1), prev_decoder_state)\n",
    "\n",
    "    if self_bank is not None:\n",
    "      _, self_context = attention(packed_batchMat, self_bank, self_bank, mask = src_mask.unsqueeze(dim=1))\n",
    "      self_bank = torch.cat((self_bank, packed_batchMat), dim=0)\n",
    "      packed_batchMat = packed_batchMat + self_context\n",
    "\n",
    "    attn, context = attention(packed_batchMat, memory_bank, memory_bank, src_mask.unsqueeze(1))\n",
    "    # logits = self.hidden2output(torch.cat((context, packed_batchMat), dim=-1))\n",
    "\n",
    "    attn = attn.squeeze(1)\n",
    "    context = context.squeeze(1)\n",
    "    packed_batchMat = packed_batchMat.squeeze(1)\n",
    "\n",
    "    decoder_states = (decoder_state, context)\n",
    "\n",
    "    ## Added:\n",
    "    logits = self.hidden2output(torch.cat([packed_batchMat, context], dim=-1))\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "      logits = torch.log_softmax(logits, dim=-1)\n",
    "    return logits, decoder_states, attn, self_bank\n",
    "\n",
    "  def evaluate_ppl(self, iterator):\n",
    "    \"\"\"Returns the model's perplexity on a given dataset `iterator`.\"\"\"\n",
    "    # Switch to eval mode\n",
    "    self.eval()\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "    for batch in iterator:\n",
    "      # Input and target\n",
    "      src = batch['src_ids']              # bsz, max_src_len\n",
    "      src_lengths = batch['src_lengths']  # bsz\n",
    "      tgt_in = batch['tgt_ids'][:, :-1] # Remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
    "      tgt_out = batch['tgt_ids'][:, 1:] # Remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
    "      # Forward to get logits\n",
    "      logits = self.forward(src, src_lengths, tgt_in) # bsz, tgt_len, V_tgt\n",
    "      # Compute cross entropy loss\n",
    "      loss = self.loss_function(logits.reshape(-1, self.V_tgt), tgt_out.reshape(-1))\n",
    "      total_loss += loss.item()\n",
    "      total_words += tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
    "    return math.exp(total_loss/total_words)\n",
    "\n",
    "  def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Switch the module to training mode\n",
    "    self.train()\n",
    "    # Use Adam to optimize the parameters\n",
    "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    best_validation_ppl = float('inf')\n",
    "    best_model = None\n",
    "    # Run the optimization for multiple epochs\n",
    "    for epoch in range(epochs):\n",
    "      total_words = 0\n",
    "      total_loss = 0.0\n",
    "\n",
    "      for batch in tqdm(train_iter):\n",
    "        # Zero the parameter gradients\n",
    "        self.zero_grad()\n",
    "        # Input and target\n",
    "        tgt = batch['tgt_ids']              # bsz, max_tgt_len\n",
    "        src = batch['src_ids']              # bsz, max_src_len\n",
    "        src_lengths = batch['src_lengths']  # bsz\n",
    "        tgt_in = tgt[:, :-1].contiguous() # Remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
    "        tgt_out = tgt[:, 1:].contiguous() # Remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
    "        bsz = tgt.size(0)\n",
    "        # Run forward pass and compute loss along the way.\n",
    "        logits = self.forward(src, src_lengths, tgt_in)\n",
    "        loss = self.loss_function(logits.view(-1, self.V_tgt), tgt_out.view(-1))\n",
    "\n",
    "        # Training stats\n",
    "        num_tgt_words = tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
    "        total_words += num_tgt_words\n",
    "        total_loss += loss.item()\n",
    "        # Perform backpropagation\n",
    "        loss.div(bsz).backward()\n",
    "        optim.step()\n",
    "\n",
    "      # Evaluate and track improvements on the validation dataset\n",
    "      validation_ppl = self.evaluate_ppl(val_iter)\n",
    "      self.train()\n",
    "      if validation_ppl < best_validation_ppl:\n",
    "        best_validation_ppl = validation_ppl\n",
    "        self.best_model = copy.deepcopy(self.state_dict())\n",
    "      epoch_loss = total_loss / total_words\n",
    "      print (f'Epoch: {epoch} Training Perplexity: {math.exp(epoch_loss):.4f} '\n",
    "             f'Validation Perplexity: {validation_ppl:.4f}')\n",
    "\n",
    "  def predict(self, tokens, K, max_T):\n",
    "    beam_searcher = BeamSearcher2(self)\n",
    "    ## Adjust tokens to fit for BeamSearcher\n",
    "    tokens = self.hf_src_tokenizer.encode(tokens)\n",
    "    tokens_length = torch.LongTensor([len(tokens)]).to(device)\n",
    "    tokens = torch.LongTensor(tokens)\n",
    "    tokens = tokens.unsqueeze(0).to(device)\n",
    "    prediction, _ = beam_searcher.beam_search(tokens, tokens_length, K, max_T)\n",
    "    # Convert to string\n",
    "    prediction = self.hf_tgt_tokenizer.decode(prediction, skip_special_tokens=True)\n",
    "    return prediction"
   ],
   "id": "65ec5d9d"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "a38ff080"
   },
   "outputs": [],
   "source": [
    "class Beam2():\n",
    "  \"\"\"\n",
    "  Helper class for storing a hypothesis, its score and its decoder hidden state.\n",
    "  \"\"\"\n",
    "  def __init__(self, decoder_state, tokens, score):\n",
    "    self.decoder_state = decoder_state\n",
    "    self.tokens = tokens\n",
    "    self.score = score\n",
    "\n",
    "class BeamSearcher2():\n",
    "  \"\"\"\n",
    "  Main class for beam search.\n",
    "  \"\"\"\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    self.bos_id = model.bos_id\n",
    "    self.eos_id = model.eos_id\n",
    "    self.padding_id_src = model.padding_id_src\n",
    "    self.V = model.V_tgt\n",
    "\n",
    "\n",
    "  def beam_search(self, src, src_lengths, K, max_T):\n",
    "    \"\"\"\n",
    "    Performs beam search decoding.\n",
    "    Arguments:\n",
    "        src: src batch of size (1, max_src_len)\n",
    "        src_lengths: src lengths of size (1)\n",
    "        K: beam size\n",
    "        max_T: max possible target length considered\n",
    "    Returns:\n",
    "        a list of token ids and a list of attentions\n",
    "    \"\"\"\n",
    "    finished = []\n",
    "    all_attns = []\n",
    "    self_bank = None\n",
    "    # Initialize the beam\n",
    "    self.model.eval()\n",
    "    #TODO - fill in `memory_bank`, `encoder_final_state`, and `init_beam` below\n",
    "    # memory_bank = ...\n",
    "    # encoder_final_state = ...\n",
    "    # init_beam = ...\n",
    "    memory_bank , encoder_final_state = self.model.forward_encoder(src, src_lengths)\n",
    "    init_beam = Beam2(encoder_final_state, torch.tensor([self.bos_id], dtype=torch.int64).to(device), 0)\n",
    "    beams = [init_beam]\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for t in range(max_T): # main body of search over time steps\n",
    "\n",
    "        # Expand each beam by all possible tokens y_{t+1}\n",
    "        all_total_scores = []\n",
    "        for beam in beams:\n",
    "          y_1_to_t, score, decoder_state = beam.tokens, beam.score, beam.decoder_state\n",
    "          y_t = y_1_to_t[-1]\n",
    "          #TODO - finish the code below\n",
    "          # Hint: you might want to use `model.forward_decoder_incrementally` with `normalize=True`\n",
    "          src_mask = src.ne(self.padding_id_src)\n",
    "          # ...\n",
    "          # logits = ...\n",
    "          # decoder_state = ...\n",
    "          # attn = ...\n",
    "          # total_scores = ...\n",
    "          logits, decoder_state, attn, self_bank = self.model.forward_decoder_incrementally(decoder_state, y_t.unsqueeze(dim=0), memory_bank, src_mask, self_bank, True)\n",
    "                                                                                #  torch.as_tensor([y_t], device=device), memory_bank, src_mask)\n",
    "          total_scores = score + logits\n",
    "          all_total_scores.append(total_scores)\n",
    "          all_attns.append(attn) # keep attentions for visualization\n",
    "          beam.decoder_state = decoder_state # update decoder state in the beam\n",
    "        all_total_scores = torch.stack(all_total_scores) # (K, V) when t>0, (1, V) when t=0\n",
    "\n",
    "        # Find K best next beams\n",
    "        # The code below has the same functionality as line 6-12, but is more efficient\n",
    "        all_scores_flattened = all_total_scores.view(-1) # K*V when t>0, 1*V when t=0\n",
    "        topk_scores, topk_ids = all_scores_flattened.topk(K, 0)\n",
    "        beam_ids = topk_ids.div(self.V, rounding_mode='floor')\n",
    "        next_tokens = topk_ids - beam_ids * self.V\n",
    "        new_beams = []\n",
    "        for k in range(K):\n",
    "          beam_id = beam_ids[k]       # which beam it comes from\n",
    "          y_t_plus_1 = next_tokens[k] # which y_{t+1}\n",
    "          score = topk_scores[k]\n",
    "          beam = beams[beam_id]\n",
    "          decoder_state = beam.decoder_state\n",
    "          y_1_to_t = beam.tokens\n",
    "          #TODO\n",
    "          # new_beam = ...\n",
    "          new_beam = Beam2(decoder_state, torch.cat((y_1_to_t, y_t_plus_1.unsqueeze(0)), dim=0), score)\n",
    "          new_beams.append(new_beam)\n",
    "        beams = new_beams\n",
    "\n",
    "        # Set aside completed beams\n",
    "        # TODO - move completed beams to `finished` (and remove them from `beams`)\n",
    "        # Set aside completed beams\n",
    "        for beam in beams:\n",
    "            if beam.tokens[-1] == self.eos_id:\n",
    "                finished.append(beam)\n",
    "                beams.remove(beam)\n",
    "\n",
    "        # Break the loop if everything is completed\n",
    "        if len(beams) == 0:\n",
    "            break\n",
    "\n",
    "    # Return the best hypothesis\n",
    "    if len(finished) > 0:\n",
    "      finished = sorted(finished, key=lambda beam: -beam.score)\n",
    "      return [token.item() for token in finished[0].tokens], all_attns\n",
    "    else: # when nothing is finished, return an unfinished hypothesis\n",
    "      return [token.item() for token in beams[0].tokens], all_attns"
   ],
   "id": "a38ff080"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "b8fd7bb0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c8dcefd4-9caa-4ba2-8bdc-d42704cfb091"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0 Training Perplexity: 4.2064 Validation Perplexity: 1.6997\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 Training Perplexity: 1.4508 Validation Perplexity: 1.3691\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:39<00:00,  2.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 2 Training Perplexity: 1.2778 Validation Perplexity: 1.2645\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:37<00:00,  2.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 3 Training Perplexity: 1.2043 Validation Perplexity: 1.2090\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:39<00:00,  2.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 4 Training Perplexity: 1.1566 Validation Perplexity: 1.1756\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:37<00:00,  2.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 5 Training Perplexity: 1.1260 Validation Perplexity: 1.1545\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:36<00:00,  2.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 6 Training Perplexity: 1.1051 Validation Perplexity: 1.1380\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 7 Training Perplexity: 1.0875 Validation Perplexity: 1.1241\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:35<00:00,  2.39it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 8 Training Perplexity: 1.0721 Validation Perplexity: 1.1197\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 9 Training Perplexity: 1.0618 Validation Perplexity: 1.1075\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:37<00:00,  2.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 10 Training Perplexity: 1.0529 Validation Perplexity: 1.1009\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:36<00:00,  2.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 11 Training Perplexity: 1.0433 Validation Perplexity: 1.0988\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 12 Training Perplexity: 1.0368 Validation Perplexity: 1.0920\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:36<00:00,  2.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 13 Training Perplexity: 1.0322 Validation Perplexity: 1.0936\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:39<00:00,  2.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 14 Training Perplexity: 1.0293 Validation Perplexity: 1.0910\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 15 Training Perplexity: 1.0250 Validation Perplexity: 1.0867\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 16 Training Perplexity: 1.0206 Validation Perplexity: 1.0904\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 17 Training Perplexity: 1.0208 Validation Perplexity: 1.0866\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:36<00:00,  2.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 18 Training Perplexity: 1.0162 Validation Perplexity: 1.0885\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 229/229 [01:38<00:00,  2.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 19 Training Perplexity: 1.0146 Validation Perplexity: 1.0914\n",
      "Validation perplexity: 1.087\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20 # epochs, we recommend starting with a smaller number like 1\n",
    "LEARNING_RATE = 1e-4 # learning rate\n",
    "checkpoint_filename = f'goal3_e_{EPOCHS}'\n",
    "# Instantiate and train classifier\n",
    "model2 = AttnEncoderDecoder2(hf_src_tokenizer, hf_tgt_tokenizer,\n",
    "  hidden_size    = 1024,\n",
    "  layers         = 1,\n",
    ").to(device)\n",
    "\n",
    "folder_name = \"models\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "if os.path.isfile(f'models/{checkpoint_filename}.pt'):\n",
    "    print(f'*** Loading model from checkpoint file {checkpoint_filename}')\n",
    "    model2.load_state_dict(torch.load(f'models/{checkpoint_filename}.pt'))\n",
    "else:\n",
    "    model2.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "    model2.load_state_dict(model2.best_model)\n",
    "    torch.save(model2.state_dict(), f'models/{checkpoint_filename}.pt')\n",
    "\n",
    "\n",
    "# model2.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "# model2.load_state_dict(model2.best_model)\n",
    "\n",
    "# Evaluate model performance, the expected value should be < 1.2\n",
    "print (f'Validation perplexity: {model2.evaluate_ppl(val_iter):.3f}')"
   ],
   "id": "b8fd7bb0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4e8d235"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Now we are ready to run the full evaluation. A proper implementation should reach more than 35% precision/recall/F1."
   ],
   "id": "c4e8d235"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "817f3005"
   },
   "outputs": [],
   "source": [
    "def seq2seq_predictor2(tokens):\n",
    "  prediction = model2.predict(tokens, K=1, max_T=400)\n",
    "  return prediction"
   ],
   "id": "817f3005"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "76f6ee2a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6969e438-64d7-4268-8547-678614d85716"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "332it [01:32,  3.58it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "precision: 0.42\n",
      "recall:    0.42\n",
      "F1:        0.42\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = evaluate(seq2seq_predictor2, test_data, num_examples=0)\n",
    "print(f\"precision: {precision:3.2f}\")\n",
    "print(f\"recall:    {recall:3.2f}\")\n",
    "print(f\"F1:        {f1:3.2f}\")"
   ],
   "id": "76f6ee2a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5779ee92"
   },
   "source": [
    "## Goal 4: Use state-of-the-art pretrained transformers\n",
    "\n",
    "\n",
    "The most recent breakthrough in natural-language processing stems from the use of pretrained transformer models. For example, you might have heard of pretrained transformers such as [GPT-3](https://arxiv.org/abs/2005.14165) and [BERT](https://arxiv.org/abs/1810.04805). (BERT is already used in [Google search](https://searchengineland.com/google-bert-used-on-almost-every-english-query-342193).) These models are usually trained on vast amounts of text data using variants of language modeling objectives, and researchers have found that finetuning them on downstream tasks usually results in better performance as compared to training a model from scratch.\n",
    "\n",
    "In the previous part, you implemented an LSTM-based sequence-to-sequence approach. To \"upgrade\" the model to be a state-of-the-art pretrained transformer only requires minor modifications.\n",
    "\n",
    "The pretrained model that we will use is [BART](https://arxiv.org/abs/1910.13461), which uses a bidirectional transformer encoder and a unidirectional transformer decoder, as illustrated in the below diagram (image courtesy https://arxiv.org/pdf/1910.13461):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/nlp-236299/data/master/img/bart_architecture.png\" alt=\"encoder-decoder illustration\" />\n",
    "\n",
    "We can see that this model is strikingly similar to the LSTM-based encoder-decoder model we've been using. The only difference is that they use transformers instead of LSTMs. Therefore, we only need to change the modeling parts of the code, as we will see later."
   ],
   "id": "5779ee92"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "398e9d94"
   },
   "source": [
    "First, we download and load the pretrained BART model from the [transformers](https://github.com/huggingface/transformers) package by Huggingface. Note that we also need to use the \"tokenizer\" of BART, which is actually a combination of a tokenizer and a mapping from strings to word ids."
   ],
   "id": "398e9d94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b698532"
   },
   "outputs": [],
   "source": [
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ],
   "id": "5b698532"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f599aab"
   },
   "source": [
    "We need to reprocess the data using our new tokenizer. Note that we use the same tokenizer for both the source and target."
   ],
   "id": "5f599aab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5271da7c"
   },
   "outputs": [],
   "source": [
    "def bart_encode(example):\n",
    "    example['src_ids'] = bart_tokenizer(example['src']).input_ids[:1024] # BART model can process at most 1024 tokens\n",
    "    example['tgt_ids'] = bart_tokenizer(example['tgt']).input_ids[:1024]\n",
    "    return example\n",
    "\n",
    "train_bart_data = dataset['train'].map(bart_encode)\n",
    "val_bart_data = dataset['val'].map(bart_encode)\n",
    "test_bart_data = dataset['test'].map(bart_encode)\n",
    "\n",
    "BATCH_SIZE = 1 # batch size for training/validation\n",
    "TEST_BATCH_SIZE = 1 # batch size for test, we use 1 to make beam search implementation easier\n",
    "\n",
    "# we use the same collate function as before\n",
    "train_iter_bart = torch.utils.data.DataLoader(train_bart_data,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True,\n",
    "                                         collate_fn=collate_fn)\n",
    "val_iter_bart = torch.utils.data.DataLoader(val_bart_data,\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       shuffle=False,\n",
    "                                       collate_fn=collate_fn)\n",
    "test_iter_bart = torch.utils.data.DataLoader(test_bart_data,\n",
    "                                        batch_size=TEST_BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                        collate_fn=collate_fn)"
   ],
   "id": "5271da7c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fae57a7"
   },
   "source": [
    "Let's take a look at the batch."
   ],
   "id": "5fae57a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b83fb1f1"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter_bart))\n",
    "src_ids = batch['src_ids']\n",
    "src_example = src_ids[0]\n",
    "print (f\"Size of text batch: {src_ids.size()}\")\n",
    "print (f\"First sentence in batch: {src_example}\")\n",
    "print (f\"Length of the third sentence in batch: {len(src_example)}\")\n",
    "print (f\"Converted back to string: {bart_tokenizer.decode(src_example)}\")\n",
    "\n",
    "tgt_ids = batch['tgt_ids']\n",
    "tgt_example = tgt_ids[0]\n",
    "print (f\"Size of sql batch: {tgt_ids.size()}\")\n",
    "print (f\"First sql in batch: {tgt_example}\")\n",
    "print (f\"Converted back to string: {bart_tokenizer.decode(tgt_example)}\")"
   ],
   "id": "b83fb1f1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36743d25"
   },
   "source": [
    "Now we are ready to implement the BART-based approach for the text-to-SQL conversion problem. In the below `BART` class, we have provided the constructer `__init__`, the `forward` function, and the `predict` function. Your job is to implement the main optimization `train_all`, and `evaluate_ppl` for evaluating validation perplexity for model selection."
   ],
   "id": "36743d25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bb56b85"
   },
   "outputs": [],
   "source": [
    "#TODO - finish implementing the `BART` class.\n",
    "class BART(nn.Module):\n",
    "  def __init__(self, tokenizer, pretrained_bart):\n",
    "    \"\"\"\n",
    "    Initializer. Creates network modules and loss function.\n",
    "    Arguments:\n",
    "        tokenizer: BART tokenizer\n",
    "        pretrained_bart: pretrained BART\n",
    "    \"\"\"\n",
    "    super(BART, self).__init__()\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    self.V_tgt = len(tokenizer)\n",
    "\n",
    "    # Get special word ids\n",
    "    self.padding_id_tgt = tokenizer.pad_token_id\n",
    "\n",
    "    # Create essential modules\n",
    "    self.bart = pretrained_bart\n",
    "\n",
    "    # Create loss function\n",
    "    self.loss_function = nn.CrossEntropyLoss(reduction=\"sum\",\n",
    "                                             ignore_index=self.padding_id_tgt)\n",
    "\n",
    "  def forward(self, src, src_lengths, tgt_in):\n",
    "    \"\"\"\n",
    "    Performs forward computation, returns logits.\n",
    "    Arguments:\n",
    "        src: src batch of size (batch_size, max_src_len)\n",
    "        src_lengths: src lengths of size (batch_size)\n",
    "        tgt_in:  a tensor of size (batch_size, tgt_len)\n",
    "    \"\"\"\n",
    "    # BART assumes inputs to be batch-first\n",
    "    # This single function is forwarding both encoder and decoder (w/ cross attn),\n",
    "    # using `input_ids` as encoder inputs, and `decoder_input_ids`\n",
    "    # as decoder inputs.\n",
    "    logits = self.bart(input_ids=src,\n",
    "                       decoder_input_ids=tgt_in,\n",
    "                       use_cache=False\n",
    "                      ).logits\n",
    "    return logits\n",
    "\n",
    "  def evaluate_ppl(self, iterator):\n",
    "    \"\"\"Returns the model's perplexity on a given dataset `iterator`.\"\"\"\n",
    "    #TODO - implement this function\n",
    "    ...\n",
    "    ppl = ...\n",
    "    return ppl\n",
    "\n",
    "  def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    #TODO - implement this function\n",
    "    ...\n",
    "\n",
    "  def predict(self, tokens, K=1, max_T=400):\n",
    "    \"\"\"\n",
    "    Generates the target sequence given the source sequence using beam search decoding.\n",
    "    Note that for simplicity, we only use batch size 1.\n",
    "    Arguments:\n",
    "        tokens: the source sentence.\n",
    "        max_T: at most proceed this many steps of decoding\n",
    "    Returns:\n",
    "        a string of the generated target sentence.\n",
    "    \"\"\"\n",
    "    # Tokenize and map to a list of word ids\n",
    "    inputs = torch.LongTensor(self.tokenizer([tokens])['input_ids'][:1024]).to(device)\n",
    "    # The `transformers` package provides built-in beam search support\n",
    "    prediction = self.bart.generate(inputs,\n",
    "                                    num_beams=K,\n",
    "                                    max_length=max_T,\n",
    "                                    early_stopping=True,\n",
    "                                    no_repeat_ngram_size=0,\n",
    "                                    decoder_start_token_id=0,\n",
    "                                    use_cache=True)[0]\n",
    "    return  self.tokenizer.decode(prediction, skip_special_tokens=True)"
   ],
   "id": "8bb56b85"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70fbe888"
   },
   "source": [
    "The code below will kick off training, and evaluate the validation perplexity. You should expect to see a value very close to 1."
   ],
   "id": "70fbe888"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adc28ae6"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5 # epochs, we recommend starting with a smaller number like 1\n",
    "LEARNING_RATE = 1e-5 # learning rate\n",
    "\n",
    "# Instantiate and train classifier\n",
    "bart_model = BART(bart_tokenizer,\n",
    "                 pretrained_bart\n",
    ").to(device)\n",
    "\n",
    "bart_model.train_all(train_iter_bart, val_iter_bart, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "bart_model.load_state_dict(bart_model.best_model)\n",
    "\n",
    "# Evaluate model performance, the expected value should be < 1.2\n",
    "print (f'Validation perplexity: {bart_model.evaluate_ppl(val_iter_bart):.3f}')"
   ],
   "id": "adc28ae6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf2f687c"
   },
   "source": [
    "As before, make sure that your model is making reasonable predictions on a few examples before evaluating on the entire test set."
   ],
   "id": "bf2f687c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd03a058"
   },
   "outputs": [],
   "source": [
    "def bart_trial(sentence, gold_sql):\n",
    "  predicted_sql = bart_model.predict(sentence, K=1, max_T=300)\n",
    "  print(\"Predicted SQL:\\n\\n\", predicted_sql, \"\\n\")\n",
    "\n",
    "  if verify(predicted_sql, gold_sql, silent=False):\n",
    "    print ('Correct!')\n",
    "  else:\n",
    "    print ('Incorrect!')"
   ],
   "id": "cd03a058"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0904585"
   },
   "outputs": [],
   "source": [
    "bart_trial(example_1, gold_sql_1)"
   ],
   "id": "f0904585"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98226b84"
   },
   "outputs": [],
   "source": [
    "bart_trial(example_2, gold_sql_2)"
   ],
   "id": "98226b84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ebdfbab"
   },
   "outputs": [],
   "source": [
    "bart_trial(example_3, gold_sql_3)"
   ],
   "id": "1ebdfbab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60969ae1"
   },
   "outputs": [],
   "source": [
    "bart_trial(example_4, gold_sql_4)"
   ],
   "id": "60969ae1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6652a685"
   },
   "outputs": [],
   "source": [
    "bart_trial(example_5, gold_sql_5)"
   ],
   "id": "6652a685"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab965fec"
   },
   "outputs": [],
   "source": [
    "bart_trial(example_6, gold_sql_6b)"
   ],
   "id": "ab965fec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3380769"
   },
   "outputs": [],
   "source": [
    "bart_trial(example_7, gold_sql_7b)"
   ],
   "id": "d3380769"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1578c156"
   },
   "outputs": [],
   "source": [
    "bart_trial(example_8, gold_sql_8)"
   ],
   "id": "1578c156"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b834df3"
   },
   "source": [
    "### Evaluation"
   ],
   "id": "2b834df3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d30379f2"
   },
   "source": [
    "The code below will evaluate on the entire test set. You should expect to see precision/recall/F1 greater than 40%."
   ],
   "id": "d30379f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5899692"
   },
   "outputs": [],
   "source": [
    "def seq2seq_predictor_bart(tokens):\n",
    "  prediction = bart_model.predict(tokens, K=4, max_T=400)\n",
    "  return prediction"
   ],
   "id": "e5899692"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7fd28e5"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1 = evaluate(seq2seq_predictor_bart, test_bart_data, num_examples=0)\n",
    "print(f\"precision: {precision:3.2f}\")\n",
    "print(f\"recall:    {recall:3.2f}\")\n",
    "print(f\"F1:        {f1:3.2f}\")"
   ],
   "id": "c7fd28e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49ba0a4b"
   },
   "source": [
    "# Discussion"
   ],
   "id": "49ba0a4b"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0039ef57"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Goal 5: Compare the pros and cons of rule-based and neural approaches.\n",
    "\n",
    "Compare the pros and cons of the rule-based approach and the neural approaches with relevant examples from your experiments above. Concerning the accuracy, which approach would you choose to be used in a product? Explain.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_comp\n",
    "manual: true\n",
    "-->"
   ],
   "id": "0039ef57"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab154d2f"
   },
   "source": [
    "**Comparing Rule-Based and Neural Approaches:**\n",
    "\n",
    "**Precision and Recall Trade-off:**\n",
    "\n",
    "-  The rule-based approach yields higher precision due to adherence to predefined grammar, while the neural approach boasts superior recall, effectively processing diverse inputs.\n",
    "\n",
    "**Training and Domain Knowledge:**\n",
    "\n",
    "- Rule-Based: No training phase is needed; however, crafting derivation rules requires domain and language understanding.\n",
    "- Neural: Requires substantial tagged data for quality outcomes but does not demand manual rule construction.\n",
    "\n",
    "**Data Requirements:**\n",
    "\n",
    "- Rule-Based: No need for tagged data.\n",
    "- Neural: Reliant on extensive tagged data for optimal performance.\n",
    "\n",
    "**Inference Time:**\n",
    "\n",
    "- Rule-Based: Inference is remarkably faster (approximately x10) compared to the neural approach due to its deterministic nature.\n",
    "- Neural: Slower inference due to iterative LSTM processing and costly matrix multiplication.\n",
    "\n",
    "**Domain Adaptation and Transfer Learning:**\n",
    "\n",
    "- Rule-Based: Tailored for specific domains like English to SQL flight queries.\n",
    "- Neural: Supports domain transfer with models like BART, enabling knowledge transfer from pre-trained models.\n",
    "\n",
    "**Deployment Considerations:**\n",
    "\n",
    "- Neural models are preferable for their ease of development and deployment, suiting multiple tasks.\n",
    "- Rule-based approaches may find application in real-time scenarios or resource-constrained environments where speed is paramount.\n",
    "\n",
    "\n",
    "**Choosing the Right Approach:**\n",
    "\n",
    "Our selection for product integration would favor the neural approach due to its versatility and practical benefits. Development and deployment are simplified, offering cost-effectiveness for multi-task scenarios. In contrast, rule-based models could be valuable for real-time applications or settings with constrained resources. The neural approach's ability to adapt across domains and leverage transfer learning further solidifies its suitability.\n",
    "\n",
    "Ultimately, the decision hinges on the project's nature, available resources, and desired performance characteristics. The neural approach aligns well with cost-effective development and deployment, making it an optimal choice for various product applications."
   ],
   "id": "ab154d2f"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b69db02"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Debrief\n",
    "\n",
    "**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on might include the following:\n",
    "\n",
    "* Was the project segment clear or unclear? Which portions?\n",
    "* Were the readings appropriate background for the project segment?\n",
    "* Are there additions or changes you think would make the project segment better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_debrief\n",
    "manual: true\n",
    "-->\n",
    "\n",
    "but you should comment on whatever aspects you found especially positive or negative."
   ],
   "id": "7b69db02"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f646c439"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ],
   "id": "f646c439"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ed02a6a"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Instructions for submission of the project segment\n",
    "\n",
    "This project segment should be submitted to Gradescope at <https://rebrand.ly/project4-submit-code> and <https://rebrand.ly/project4-submit-pdf>, which will be made available some time before the due date.\n",
    "\n",
    "Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion. You should submit your code to Gradescope at the code submission assignment at <https://rebrand.ly/project4-submit-code>. Make sure that you are also submitting your `data/grammar` file as part of your solution code as well.\n",
    "\n",
    "We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope at <https://rebrand.ly/project4-submit-pdf>."
   ],
   "id": "6ed02a6a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f0d9f46"
   },
   "source": [
    "# End of project segment 4"
   ],
   "id": "3f0d9f46"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "title": "CS236299 Project Segment 4: Semantic Interpretation – Question Answering",
  "vscode": {
   "interpreter": {
    "hash": "4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4dee71169f684f2e88014fcaf29708f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1cfa79d944f446dba90cc71fdda893dd",
       "IPY_MODEL_a0d768076952436aa3124db53109eb61",
       "IPY_MODEL_5f430e1bce414d4a93bbf1055969f498"
      ],
      "layout": "IPY_MODEL_efddd5126bea46508e15099d62ed7873"
     }
    },
    "1cfa79d944f446dba90cc71fdda893dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f56d9316ca5842d1a552e3b40d5f2956",
      "placeholder": "​",
      "style": "IPY_MODEL_7762395d366744068a5c9dacd2aca400",
      "value": "Downloading data files: 100%"
     }
    },
    "a0d768076952436aa3124db53109eb61": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30bd96363cdf4f2e8569f96b2b03968b",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_19e33c9b88e5454db4719512ae002cda",
      "value": 3
     }
    },
    "5f430e1bce414d4a93bbf1055969f498": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad9654e7d35e4b368ea64912dfea76f8",
      "placeholder": "​",
      "style": "IPY_MODEL_00354d752dfd4c6f97c9a0a522c02dcd",
      "value": " 3/3 [00:00&lt;00:00, 98.59it/s]"
     }
    },
    "efddd5126bea46508e15099d62ed7873": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f56d9316ca5842d1a552e3b40d5f2956": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7762395d366744068a5c9dacd2aca400": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "30bd96363cdf4f2e8569f96b2b03968b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19e33c9b88e5454db4719512ae002cda": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ad9654e7d35e4b368ea64912dfea76f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00354d752dfd4c6f97c9a0a522c02dcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae99dd7c4f884f7ab8b83fed15d4d86c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac79489695f348aca3d355fa5716fb34",
       "IPY_MODEL_950efd1d6326491c9f87b0471e99601c",
       "IPY_MODEL_81fb3951333b41e7acced4bcd370e0da"
      ],
      "layout": "IPY_MODEL_96e288bbd6c143c7b174ea1fc77e4489"
     }
    },
    "ac79489695f348aca3d355fa5716fb34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6968fb739679425abff833d4dee61361",
      "placeholder": "​",
      "style": "IPY_MODEL_c14a234fd84e4a70903e6b0a9ceeb5ae",
      "value": "Extracting data files: 100%"
     }
    },
    "950efd1d6326491c9f87b0471e99601c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84c6ee75b7e642b58477da0bd2520049",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8195fef7e151400e8719fef71cfa0f49",
      "value": 3
     }
    },
    "81fb3951333b41e7acced4bcd370e0da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93aa9223c9014bc9b53e1f5095a25783",
      "placeholder": "​",
      "style": "IPY_MODEL_a31dea566d904677ad82ce5737934828",
      "value": " 3/3 [00:00&lt;00:00, 87.94it/s]"
     }
    },
    "96e288bbd6c143c7b174ea1fc77e4489": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6968fb739679425abff833d4dee61361": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c14a234fd84e4a70903e6b0a9ceeb5ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84c6ee75b7e642b58477da0bd2520049": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8195fef7e151400e8719fef71cfa0f49": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "93aa9223c9014bc9b53e1f5095a25783": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a31dea566d904677ad82ce5737934828": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df7813c20694472eb4e865d476615d90": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4c4e5cbcf1140a8bfeeda3fc42e0582",
       "IPY_MODEL_e884488e81c040beb57b0af5921a9c3f",
       "IPY_MODEL_30fee2aaf20c4388b5e0c6cceff5b9d0"
      ],
      "layout": "IPY_MODEL_4c6bf6786cc3498887732e4e4d7edcb9"
     }
    },
    "b4c4e5cbcf1140a8bfeeda3fc42e0582": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5bf63a9831114ed59e0b4f655c2b0b94",
      "placeholder": "​",
      "style": "IPY_MODEL_cbb8257b2914471f82b896927803964f",
      "value": "Generating train split: "
     }
    },
    "e884488e81c040beb57b0af5921a9c3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2cbf6c4a4e64113b320928c0ed24e5a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07a84d3050684101a4c0d96cf13304f5",
      "value": 1
     }
    },
    "30fee2aaf20c4388b5e0c6cceff5b9d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be6bb989d048445a9c3fd1d908017f6d",
      "placeholder": "​",
      "style": "IPY_MODEL_b6b13cdf776243b1b1f6b69324b0eba3",
      "value": " 3651/0 [00:00&lt;00:00, 30373.25 examples/s]"
     }
    },
    "4c6bf6786cc3498887732e4e4d7edcb9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "5bf63a9831114ed59e0b4f655c2b0b94": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbb8257b2914471f82b896927803964f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2cbf6c4a4e64113b320928c0ed24e5a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "07a84d3050684101a4c0d96cf13304f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "be6bb989d048445a9c3fd1d908017f6d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6b13cdf776243b1b1f6b69324b0eba3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "292cc79415624d31927e0c0158804314": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2ec338c61b03411883d8d20cb93d4628",
       "IPY_MODEL_b34535e0219143eca809fad32ef90a78",
       "IPY_MODEL_201759a49d4549eda5f8a5eaa430d5b9"
      ],
      "layout": "IPY_MODEL_1752c94ecb7d4966a32e2037ec8e8eb6"
     }
    },
    "2ec338c61b03411883d8d20cb93d4628": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e880523367e4cc290e70c1d04e76187",
      "placeholder": "​",
      "style": "IPY_MODEL_18e41402e7b040e3bc404496102e3000",
      "value": "Generating val split: "
     }
    },
    "b34535e0219143eca809fad32ef90a78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73388a6ca11c45399980dc8a7c3bd78c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ef0f1e963c84205a74d08b544c6de61",
      "value": 1
     }
    },
    "201759a49d4549eda5f8a5eaa430d5b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ff987d03525421183945dbbc0b2581a",
      "placeholder": "​",
      "style": "IPY_MODEL_8cf2a421d5ea42ff805d481a63c1dd44",
      "value": " 0/0 [00:00&lt;?, ? examples/s]"
     }
    },
    "1752c94ecb7d4966a32e2037ec8e8eb6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "6e880523367e4cc290e70c1d04e76187": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18e41402e7b040e3bc404496102e3000": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73388a6ca11c45399980dc8a7c3bd78c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7ef0f1e963c84205a74d08b544c6de61": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ff987d03525421183945dbbc0b2581a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cf2a421d5ea42ff805d481a63c1dd44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db80abd25c674a6680f3adef61315673": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fc84d0d8eb84763920785ff056296fc",
       "IPY_MODEL_8c37a61a29614dadafa95f6c66d8eea6",
       "IPY_MODEL_ae7d3f2982c54a98ba1584501d98d35e"
      ],
      "layout": "IPY_MODEL_e7fa8cd383d54aa787373e0adfc27be3"
     }
    },
    "3fc84d0d8eb84763920785ff056296fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4350273089f47ed97f839ad0473d65e",
      "placeholder": "​",
      "style": "IPY_MODEL_3ffb61197118488585f3bc58f4ac5782",
      "value": "Generating test split: "
     }
    },
    "8c37a61a29614dadafa95f6c66d8eea6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d6cd59136864119b69168c83fd86d8f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5057b43ed0dc4a3692a3473198c28c2c",
      "value": 1
     }
    },
    "ae7d3f2982c54a98ba1584501d98d35e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1035fdd02a964e34960a9ccacd1c19e9",
      "placeholder": "​",
      "style": "IPY_MODEL_be0086dbebfd40fc889847138ddf3cde",
      "value": " 0/0 [00:00&lt;?, ? examples/s]"
     }
    },
    "e7fa8cd383d54aa787373e0adfc27be3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "b4350273089f47ed97f839ad0473d65e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ffb61197118488585f3bc58f4ac5782": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d6cd59136864119b69168c83fd86d8f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "5057b43ed0dc4a3692a3473198c28c2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1035fdd02a964e34960a9ccacd1c19e9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be0086dbebfd40fc889847138ddf3cde": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8e39e0ac7b64dbe9698cd9880b607e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d030d51c3534ab4ac79687bff877e87",
       "IPY_MODEL_d023c306c6b54da5a351bf9c6c6c8bdc",
       "IPY_MODEL_c4302cb2529c479ab1a058cf92f8543c"
      ],
      "layout": "IPY_MODEL_8ee814c68db04b94bbde751a8a332ab6"
     }
    },
    "8d030d51c3534ab4ac79687bff877e87": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc22ebb9b7a14670ac4932473f3c0de2",
      "placeholder": "​",
      "style": "IPY_MODEL_8dc0fac98db8441f96ce3679847b53c6",
      "value": "100%"
     }
    },
    "d023c306c6b54da5a351bf9c6c6c8bdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_884607c5e45d4ad6a501e33d9f12b002",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b7c50a5c7b524c34a3583dde2350f17c",
      "value": 3
     }
    },
    "c4302cb2529c479ab1a058cf92f8543c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaed3ae2aa714dc8b7ab14b241527da3",
      "placeholder": "​",
      "style": "IPY_MODEL_d31354e47a5e44e4bc53abec349160d2",
      "value": " 3/3 [00:00&lt;00:00, 94.59it/s]"
     }
    },
    "8ee814c68db04b94bbde751a8a332ab6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc22ebb9b7a14670ac4932473f3c0de2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dc0fac98db8441f96ce3679847b53c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "884607c5e45d4ad6a501e33d9f12b002": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7c50a5c7b524c34a3583dde2350f17c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aaed3ae2aa714dc8b7ab14b241527da3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d31354e47a5e44e4bc53abec349160d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "522f3e8901974611ae4b189ce991bfbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4cebe6d2681846c79a438cdd5b29a661",
       "IPY_MODEL_863bed341ca34d098ae52e6b8db33040",
       "IPY_MODEL_6f209b9ecc3d4baa8f22519cff0954e9"
      ],
      "layout": "IPY_MODEL_2bc42955d4334e5c82f5a99a95eb08f8"
     }
    },
    "4cebe6d2681846c79a438cdd5b29a661": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c678a562d4534be297137ce9871eaa90",
      "placeholder": "​",
      "style": "IPY_MODEL_0d828a29493c4bb7a78e8eaa88bbdb94",
      "value": "100%"
     }
    },
    "863bed341ca34d098ae52e6b8db33040": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5471c56871bf4cc089b61a7dde8c050a",
      "max": 3651,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5126ccc1ca3145748fff13916cfbec2c",
      "value": 3651
     }
    },
    "6f209b9ecc3d4baa8f22519cff0954e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecb0aa5b1af643e8aa9c2d0d30358bca",
      "placeholder": "​",
      "style": "IPY_MODEL_9958ba66414d43df874eb3123932068a",
      "value": " 3651/3651 [00:01&lt;00:00, 1752.24ex/s]"
     }
    },
    "2bc42955d4334e5c82f5a99a95eb08f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c678a562d4534be297137ce9871eaa90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d828a29493c4bb7a78e8eaa88bbdb94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5471c56871bf4cc089b61a7dde8c050a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5126ccc1ca3145748fff13916cfbec2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ecb0aa5b1af643e8aa9c2d0d30358bca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9958ba66414d43df874eb3123932068a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1343a07fc8a348d3aa8fe0f47e52db8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2e4fbc92ad3b48ff8615332a6eda4a8e",
       "IPY_MODEL_1c1f5e914f374b99807fca96139fb028",
       "IPY_MODEL_cd94788d00714fddb7a1f38d2e31b79d"
      ],
      "layout": "IPY_MODEL_0a36f7df7d9d4253a626123d3a2ff3a3"
     }
    },
    "2e4fbc92ad3b48ff8615332a6eda4a8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8aa163c29704d5889bbc5fdec44634d",
      "placeholder": "​",
      "style": "IPY_MODEL_7f6f923d36854bf294245ae4b08b5d36",
      "value": "100%"
     }
    },
    "1c1f5e914f374b99807fca96139fb028": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72ae0700c19447289b8512ebe699ed55",
      "max": 398,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d1c80f56403a4d9d8a3191c4fdd52a1c",
      "value": 398
     }
    },
    "cd94788d00714fddb7a1f38d2e31b79d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_254b957a206c48a3978039a9b30f2f94",
      "placeholder": "​",
      "style": "IPY_MODEL_468d6e3b9ea7425fa0e37a828e9cfadb",
      "value": " 398/398 [00:00&lt;00:00, 1015.30ex/s]"
     }
    },
    "0a36f7df7d9d4253a626123d3a2ff3a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8aa163c29704d5889bbc5fdec44634d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f6f923d36854bf294245ae4b08b5d36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72ae0700c19447289b8512ebe699ed55": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1c80f56403a4d9d8a3191c4fdd52a1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "254b957a206c48a3978039a9b30f2f94": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "468d6e3b9ea7425fa0e37a828e9cfadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fd96144edd941779e31d3fb807ca7df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_82997ab5b88345b2bfda4a3eb49aa34d",
       "IPY_MODEL_c553be0486d04e49b232984b9b9a0eb2",
       "IPY_MODEL_d838801205664038974c26a4f88fa76c"
      ],
      "layout": "IPY_MODEL_2903812e68b94a3ba9d78673bb785d0f"
     }
    },
    "82997ab5b88345b2bfda4a3eb49aa34d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcf2a64d93694d2083fe229391b480d2",
      "placeholder": "​",
      "style": "IPY_MODEL_62e11e1cffa14e658f3449adcc46fe65",
      "value": "100%"
     }
    },
    "c553be0486d04e49b232984b9b9a0eb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9ff18e6f8da409c8f0d17d07abecde2",
      "max": 332,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63e0005b21304a4d92b36182bdec023b",
      "value": 332
     }
    },
    "d838801205664038974c26a4f88fa76c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_713e31dbab5b4625a0f108af261ae37f",
      "placeholder": "​",
      "style": "IPY_MODEL_5d595b58fb5f492897ebf48388c2554c",
      "value": " 332/332 [00:00&lt;00:00, 1123.39ex/s]"
     }
    },
    "2903812e68b94a3ba9d78673bb785d0f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcf2a64d93694d2083fe229391b480d2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62e11e1cffa14e658f3449adcc46fe65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a9ff18e6f8da409c8f0d17d07abecde2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63e0005b21304a4d92b36182bdec023b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "713e31dbab5b4625a0f108af261ae37f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d595b58fb5f492897ebf48388c2554c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
